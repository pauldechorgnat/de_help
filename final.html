<script>
    next_slide = function(element, number_of_images){
        console.log(element);
        var image_name = element.src.match('image[0-9]+\\.png')[0];
        var new_image_number = (Number.parseInt(image_name.match('[0-9]+')) + 1) % number_of_images;
        var new_image_path = 'image' + new_image_number + '.png';
        element.src = element.src.replace(RegExp('image[0-9]+\\.png'), new_image_path);
        return false;
    }
</script>
<link rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>

<script>
hljs.configure({useBR: true});
document.addEventListener('DOMContentLoaded', (event) => {
  document.querySelectorAll('blockquote code').forEach((block) => {
    hljs.highlightBlock(block);
  });
});
</script>
<script>
    evt = document.createEvent("HTMLEvents");
    evt.initEvent('DOMContentLoaded');
    document.dispatchEvent(evt);
</script>
<script>
    function copyText(element) {
      var range, selection, worked;

      if (document.body.createTextRange) {
        range = document.body.createTextRange();
        range.moveToElementText(element);
        range.select();
      } else if (window.getSelection) {
        selection = window.getSelection();
        range = document.createRange();
        range.selectNodeContents(element);
        selection.removeAllRanges();
        selection.addRange(range);
      }

      try {
        document.execCommand('copy');
        alert('text copied');
      }
      catch (err) {
        alert('unable to copy text');
      }
    }
</script>
<style>
    body {
        font-family: Source Sans Pro,sans-serif;
        }
    .slider {
        width:80%;
        margin-left: auto;
        margin-right: auto;
        height:auto;
    }
</style>

<p><hr style="border-color:#75DFC1; width:80%;"/><center></p>

<h1>Distributed Architectures, Hadoop and tools</h1></center>
<hr style="border-color:#75DFC1; width:80%;"/>

The volume of data created is exponentially increasing: to get a sense of this volume, each minute 103 million spam emails, 3.6 million Google searches, 450 000 tweets are exchanged. The forecast are that this volume will be multiplied by 10 in 2025. 

In order to draw value from those data, we need to be able to store and exploit them. 


<h2>Distributed Architectures vs Classical Architecture</h2>

When faced to the problem of Big Data storage, there are mainly two ways to increase one's storing capacities or computing power:


md5-635d90cd3403e7c53fe3c98091a375bd


<h3>Some definitions </h3>
A **cluster** is a set of machines also called **nodes**. Those machine can be linked to each other or have special network architectures. 
Generally speaking some machines are more important than other: they are more powerful and are thus more oriented toward orchestration, planning, ... <br/>Those are called **master** nodes while the regular one are called **worker** or **slave**: they are the one actually doing the computations and the storing.

<h3>Distributed architectures are cheaper</h3>
For decades, improvements on transistor capacities, hence on storage and computing facilities have been increasing exponentially, following Moore's law. Lately, due to physical limitations, we witnessed a downturn in this trend.  
It is now cheaper to buy cheap hardware, also called commodity hardware, in number than buying the largest possible machine. For the same computer power it is cheaper to use distributed architectures than very powerful single machines.

### Distributed architectures are more secured
There are two important aspect in distributed architectures: 


md5-89d2e6bb7c379e8e7b83096112baa56d



@slider partition_replication_slider

At first you may think that this increases the demand for storage, which is true, but remember that storage is very cheap while data is key to business today. This partition/replication increases the security of the data. We cannot assume that the crash of a machine is an extraordinary event: this is an event that is going to happen so we need to have copies of the data that are available at any time:

@slider redundancy_security_slider

<i>In this example, a document is spread on a cluster of 6 machines. It is very simplified but we have to have 3 machines down before not being able to access this document. The likeliness of this happening in a time so short that no one can react is low. Moreover tools are designed to see when a machine is down and take action so that the data is replicated on another machine so that we have all our replicas in the cluster.<br/>
In some companies where data is a very sensible issue, data is replicated inside data centers which are themselves replicated in different countries in order to avoid natural disasters (floods, earthquake, ...) or man made phenomena (attacks, war, ...).</i>
<h3>Distributed architectures speeds up computing</h3>

Partitioning data allows different machine to work on the same document at the same time. This is a fundamental  aspect of distributed architectures: since documents are spread over different machine, those machines can work in parallel at the same time. Moreover, since the data is already stored in small computable chunks on different machines, we do not need to move data from a machine to another: we just have to pick available machines that have the data. 

@slider distributed_computing_slider

<h3>But distributed architectures face some problems</h3>

As you might have guessed by now, the organisation of the cluster is very important and needs to be managed well: the cluster needs to know where pieces of data are stored, which machines are available to compute and how the results of a computation should be handled: this task is not trivial and that is why we have some tools such as <i>Hadoop</i> to perform this part. 

Moreover, the <b>CAP theorem</b> limits the possibilities of distributed systems: this theorem states that you cannot achieve perfect <b>availability</b> or perfect <b>consistency</b> of your data if your data is spread across a cluster.

<i>Think for example of a company that allows you to book airplane tickets. The availability of a given seat is going to be stored on different nodes at the same time. If Alice books the seat on a node, the information needs some time to be updated on the other nodes. If Bob wants to book the same seat, there are two possibilities:


md5-b1ae50c2dbddde1e5c48e619de217876


</i>

@slider cap_theorem_slider 

<b>CAP</b> actually stands for <b>Consistency</b>, <b>Availability</b> and <b>Partition Tolerance</b>: the theorem states that you cannot have the three at the same time. 

<h2>Map Reduce</h2>

MapReduce is a programming model used widely in distributed systems to perform calculations. It has been patented by Google in 2004. MapReduce relies on the emission of key-value pairs during a Map phase and the aggregation of the values by keys during the Reduce phase.

Every partition of the data undergoes the same transformation, emitting a set of key-value pairs. This is the <b>Map</b> phase. Those pairs are emitted to a machine based on the key of the pair.

For example every pair with the key <code>key_1</code> is sent to a specific machine no matter how many keys there are. There can be different keys on a single machine but every value corresponding to a key need to be on the same machine. Creating the key-value pairs is done during the <b>Map</b> phase while the sorting and dispatching of values is called the <b>Shuffle</b> phase.

Once all sent to the right machine, values are aggregated by key to get the final result: this is the <b>Reduce</b> phase. 

<h3>WordCount</h3>
To explain how Map Reduce works, the usual example is to take the `WordCount` task, e.g. computing the frequencies of words. It is the <code>print('hello world')</code> of MapReduce.

During the <b>Map</b> phase, the partitions of the text are lowered, tokenized and for each token, we emit the token as the key and 1 as the value.

The pseudo-code for this step is the following:



md5-8e888bd3f3a2fecf0106b5f6d0f536bb



Remember that the <b>Shuffle</b> phase forces all values corresponding to the same key to be on the same reducing machine. So on every machine used during the <b>Reduce</b> phase, we have a set of keys, corresponding to some tokens present in the text and for each key a list of ones of length corresponding to the number of instances of this token over the partitions. 

The pseudo-code for the <b>Reduce</b> is given in the following block: the code is given only for one key because during the <b>Reduce</b>, keys are treated totally independently.



md5-afad2fa1647e6d29fa673575ce281b6c



@slider mapreduce_wordcount_slider

The succession of a <b>Map</b>, <b>Shuffle</b> and <b>Reduce</b> phases is called a <b>Job</b>. We can combine multiple Jobs to perform complicated tasks. The number of reducers and mappers is up to the operator: if they is not a mapper by partition, some mappers will will take care of multiple partitions sequentially. On the other hand, having too many machines involved in an operation, some may not be available for other operations in the same time.

<h3>Advanced MapReduce: Combiners</h3>

You may have noticed that during the Map phase of Wordcount, we have emitted the same keys twice on the same mapper machine. This can be problematic as this may increase significantly the number of values that are emitted. 

We can aggregate those values during the <b>Map</b> phase using a <b>Combiner</b>: this can be considered as a <b>Reduce</b> phase in the mapper. If we take the example of Wordcount, the new pseudo-code for the <b>Map</b> is the following: 



md5-2d00e46726287978f70dee1a431f67c9



@slider wordcount_combiner_slider

<h3>Advanced MapReduce: Partitioners</h3>

During the <b>Shuffle</b> phase, the values are sent to specific machines according to the associated keys. The default rule is generally: 

<center><code> number of the reducer machine(key) = HASH_FUNCTION(key) MOD number of reducer machines</code></center>

This ensures an even distribution of keys over reducer nodes: each machine should receive about the same number of different keys. The issue with that is that in some particular cases, some keys can be over-represented. This can create a bottleneck which will slow down the execution of the jobs.  

@slider mapreduce_partitioners_slider

<i>In this example, the key <code>key1</code> is over-represented. <br/>If we let the default partitioner take care of the <b>Shuffle</b>, then one of the reducer is going to receive much more data to process than the other but if we take into account the a priori knowledge of the distribution of values, we can define a partitioner that will balance the workload over the reducers.</i>

Partitioners cannot be coded dynamically: you have to know a priori the balance of values by key to implement one that will balance evenly the workload.

<h2>Apache Hadoop</h2>

Apache Hadoop is an Open Source framework for distributed architectures. It was first released in 2006 by Yahoo! but is now an Apache Foundation project. It is a widely used tool in Big Data to manage distributed storage and computing. 

<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Hadoop.png"/></center>
<h3>Hadoop components</h3>

Hadoop is mainly composed of 3 components: 


md5-9d13dd597fc3be47eaf9edd8ca80493e



While it is coded in Java, Hadoop allows also to use other programming languages by writing in and parsing the standard output directly. This allows us to code MapReduce Jobs in Python, Ruby, or any other language given that you can read from the standard output. This utility is called <b>Hadoop Streaming</b>.

<h4>HDFS</h4>

As stated before, <b>HDFS</b> is in charge of distributed storage. There are multiple daemons running in HDFS: 


md5-495ecfb4267ad54cf674698e5cb847a5


<h4>YARN</h4>
<b>YARN</b> is in charge of orchestrating work and jobs. There are several daemons running within YARN: 


md5-cfb84dd1068b2a2966497a69e48dd162


<center>?????????????????????????????????????????????????????????????????????????????????????????????????????????</center>


The code of an application contains information about the location of the files (in HDFS or locally), about the number of mappers and reducers and of course the code of Mappers, Combiners, Partitioners and Reducers. 

When it is launched, the <b>Resource Manager</b> triggers an <b>Application Manager</b> on one of the worker nodes. The <b>Application Manager</b> knows the resource needed (number of mappers and reducers) and asks the <b>Resource Manager</b> for those resources. 

The <b>Resource Manager</b> launches containers that includes a <b>JVM</b> and the code of the mappers and the reducers and communicates the address and ids of the containers to the <b>Application Manager</b>. 

Then, those containers communicate to the <b>Application Manager</b> which is in charge of orchestrating the jobs. The communication goes through the <b>Resource Manager</b> because <b>Hadoop</b> is based on a share-nothing model, e.g. worker nodes are only linked to the master nodes not one to each other. This leaves the <b>Resource Manager</b> available for other applications to be run on other nodes. 



<h2>Installing Hadoop</h2>
<i>In this part, we are going to install <b>Hadoop</b> in a pseudo-distributed mode. The components have already been downloaded but the setting must be done.</i>
<h3>Installation modes</h3>

There are 3 available installation modes: 


md5-4452c374fbec6239a92daeb0baf7d8dc


<h3>Configuration files</h3>

The installation files are located under the archive <code>/home/ubuntu/hadoop-2.7.3.tar.gz</code>. 
<i>It has been downloaded from <a href="https://hadoop.apache.org/releases.html">Hadoop</a> website. </i>

First, we need to decompress the archive:


md5-50a5d8b06f5b37d1eb6843e7cb8d3cee



To check that it indeed has been decompressed, you can use the command: 


md5-60a158288832b6d8cbca64e04a159733




You should see both the archive and the folder with the same name (except the extension).


Now we need to edit the <code>/home/ubuntu/.bashrc</code> file to give it paths to Hadoop files. 

<i>We are going to use <code>nano</code> but you can use any text editor you want.</i>


md5-4060ad5c5ee46f16055ad6fa769328f9



Append those lines to the file: 



md5-8b7e0d179a3be51abef6420e5b3e41bd



Close the file and commit those changes with the <code>source</code>: 



md5-7d80679fd471da527d4d93e76bfe8164



We are going to check that <b>Java</b> and <b>Hadoop</b> are installed: 



md5-8581a309676b31fb70ac52382a8b7e38



You should see the version of the two softwares. 

We are now going to shape the cluster. 

There are several files that need to be edited. 

<h4><b>core-site.xml</b></h4>
This file contains settings for the <b>namenode</b>. The namenode address will be on local port 9000. 

Open the file: 



md5-d5fb75b03288646820098276568b4c0e



Within the tags <code>&lt;configuration</code>, paste the following lines:



md5-71149756128763e7fbd18ec8f46c5158


<h4><b>hdfs-site.xml</b></h4>
This file contains information about how HDFS functions: we are going to choose a replication factor of 2, e.g. there will be 3 copies of each file. We will specify also a local directory to store namenode data and the datanode data. Finally, we need to tell him not to check for security clearance at each action. 

Open the file:



md5-cd02514dd6653e42f32a57fa5c932e63



Within the tags <code>&lt;configuration&gt;</code>, paste the following lines:



md5-fffdb1d3fc6942cb5c1b0252c5f042ba


<h4><b>mapred-site.xml</b></h4>

In this file, we will simply state that the resource manager that will be used is YARN.

First, we need to copy/paste the template of this configuration file: 



md5-edcecca0fb29fa91aef80313508ad663



Open the file:



md5-aca2abd19a4f851c18305a6de8b8f6cf



Within the tags <code>&lt;configuration&gt;</code>, paste the following lines:



md5-8eaec09309ecd564797685fb71421ee1


<h4><b>yarn-site.xml</b></h4>

This files contains <b>YARN</b> settings. We simply tell him what <b>Java</b> classes should be used for the shuffle step: 

Open the file:



md5-d1a775088780dd0f87fe97025b480de1



Within the tags <code>&lt;configuration&gt;</code>, paste the following lines:



md5-4dc4bf6f57d98509bbebd6e45ba61cef


<h4><b>hadoop-env.sh</b></h4>
In this file, we will just specify where <b>Hadoop</b> can find <b>Java</b>.

Open the file: 



md5-6565b2962d07545282927141275e6bd5



Append this line to the file:



md5-5b737c0487910f22a9e758d17829b8d2




One last thing to do is to generate SSH keys: 



md5-f23ab3d1601f8b89fe6c5fdbf1426d60



Follow the default instructions (no password and default name) then add the key to the authorized keys: 


md5-da4fef75a8fbbe6aaf75e6ebeef2daff


<h3>Initialization</h3>


First we need to format the namenode: 



md5-cee26f662d8e5ec568dcec0df1d7c219



You can check that a folder has been created by using the following command:



md5-8271c34f0578fe48a1388f6dace4ddf5



Now we can start the daemons for <b>HDFS</b>: 



md5-ca7f268c1476157c25ec067ca236e7c7



You can see which <b>Java</b> processes are running at anytime by using the command <code>jps</code>.

If you run this command here, you should see: 


md5-483e97e571a5101bb30287cd5172fb20



Now we can start <b>YARN</b> daemons:



md5-a760b81c7663cc941c732c6f125cdadd



If you run <code>jps</code>, you should see: 


md5-6cde5a5146e0019613fbf2f11744a03d


<h3>HDFS practice</h3>

For this part, we have downloaded a book from <a href="https://www.gutenberg.org/">Project Gutenberg</a> library: The Adventures of Sherlock Holmes. It is a simple text file available on your local file system at <code>/home/ubuntu/datasets/books/sherlock_holmes.txt</code>. You can check the headers of the book with the following command: 



md5-2c700b6f5bd7c05a0fbb34b83b16d9a2



Most of the commands that we are going to use are based on the same pattern: <code>hdfs dfs -...</code> and we simply to write those commands into the shell. 

First, let's create a folder named <code>data</code> in our distributed file system: 



md5-cc3f95b9b9bcfa2a11302571a89efe30



You can check that the folder has indeed been created in the distributed file system and not in the local one by checking both systems at the root: 

<b>Local file system</b>


md5-fe50ca07218d00e396531ad971fd4e45


<b>HDFS</b>


md5-32f09dfc41cfcd4d6b3c63fc7e489679


<code class="bash">hdfs dfs -ls</code> is used in the same way <code>ls</code> is used in a usual shell. HDFS is organised as any UNIX file sytem, starting from the root <code>/</code>. 

We can also use the <code>-R</code> argument to have a recursive view of the folders: 
<code class="bash">hdfs dfs -ls -R /</code>. 

Notice that we always need to specify the absolute path as there are no current directory in <b>HDFS</b>.


We are now about to put our book into <b>HDFS</b>: 



md5-ad78695d4c35348a3216a3a3723bc54a



We can also use <code>-copyFromLocal</code>


md5-924334936866bd0fd7830501fce98091



The main difference between those two commands is that <code>-put</code> can handle multiple files at once while <code>-copyFromLocal</code> cannot. 

The syntax is: 


md5-9a75ff53a05d74ad03afa0661d8018cb



This command is very similar to <code>cp</code> or <code>scp</code>.

We can check that the file is indeed here: 



md5-60772d8d1e5227ce282d6c5ff0aea60e



Or print its content using <code>-cat</code>: 



md5-fcaec3c960634e1eed2f074756e39dc6



The contrary can be done using <code>-get</code>: 


md5-182073d39494fbb83603a347908a4a19





Of course we can do a lot of the usual commands of a filesystem: 



md5-72a70658a1c9e58503811b8bdb0bd6bc



We can also use regular expressions to address multiple files at the same time. 

<h4>User Interface</h4>

HDFS has also a way to visualize the file system information: if you open a window on the 50070 port, you should be able to see the UI. 
For example, you can see your files in <code>Utilities&gt;Browse the file system </code>.


<h4>Exercise</h4>
<i>In the folder <code>/home/ubuntu/books</code> there are <code>moby_dick.txt</code> and <code>alice.txt</code>. Try to do the following:


md5-b09d59fa42331811cc48f174292ca159


</i>
<h3>MapReduce practice</h3>


We are going to execute a <b>MapReduce</b> job performing wordcount. 

<h4>Hadoop MapReduce</h4>

In this first part, we will perform a <b>WordCount</b> on <code>sherlock_holmes.txt</code> using the <b>Hadoop MapReduce Java</b> library. The application is already coded and compiled in the file <code>wordcount.jar</code> but it is available <a href="lien-vers-wordcount.java">here</a>. Feel free to read this file to get the pattern on <b>MapReduce</b> job implementation. You'll note that we did not specify a number of mappers and reducers, so there will be only one of each by default. 

To run this file, the syntax is the following: 



md5-e5a65243657d024838f57cb4146256e4



More generally, to perform a <b>MapReduce</b>, we can do the following:



md5-83bb08f8c0c53159b6e84c7e624d551b



Here the two arguments are the input file and an output folder. Note that this folder should not exist before running this code. 
Once run, you can see that Hadoop is very wordy: there are a lot of information and it is sometimes difficult to read through it. 

Now we can check the result of our job: 



md5-73918307416a19a7661e7488f8037152



As you can see a file named <code>part-r-00000</code> has been created. It contains the result of the Job. 



md5-097043f4f0d4861ce841f0bb4ae0a1b1



This name corresponds to the fact that we have the output of a single reducer. If we had multiple reducers, we would have different files corresponding to the different partitions of our results with names incremented by 1 for each reducer. 

<h3>Hadoop Streaming with Python</h3>

As stated before, <b>Hadoop Streaming</b> allows us to perform <b>MapReduce</b> jobs using other languages than <b>Java</b>. 

In this part we are going to run a <b>MapReduce</b> job using <b>Python</b>.
In the folder <code>/home/ubuntu/code</code>, you can find <code>mapper.py</code> and <code>reducer.py</code>. Feel free to read their content: we are basically doing the same thing as before but with <b>Python</b> we are only printing out results in the standard output. Note also that we need to specify which command is used at the beginning of the file. 

Before running the command, we are just going to alias the command: this will ease the command. 

Open <code>.bashrc</code>:



md5-4060ad5c5ee46f16055ad6fa769328f9



Append the following lines to the file: 



md5-f74682a0d8a36aca34565a5e0a46820f



Do not forget to commit the changes to this file with <code>source ~/.bashrc</code>.

You can run the job by doing the following command: 



md5-1336118f16157a010c830fe82bad8745



We have to specify the code files twice because they are not hosted on the distributed file system. Moreover the output argument is still a non-existing folder in <b>HDFS</b>.

You can check the results: 



md5-34fcc45c4c30bcd104719cc269af549b



The code is a bit simpler than before so the results are a bit rougher. Yet we get the same idea. 

<h4>Exercise</h4>
<i>In this part, we will use the file <code>sentiment.csv</code> in the folder <code>/home/ubuntu/datasets</code>. This file contains two columns: the first represents the text of tweets while the second represents the sentiment (0 is for negative, 1 for positive). 

You shall do the following: 


md5-2d9984357e63a99fac665beddbeaaae3


</i>
<h3>Conclusion on Hadoop</h3>

In this part, we have seen the importance of distributed systems and the advantages it presents compared to classical architectures. We have also seen how to code in a framework like <b>MapReduce</b> and how to use specifically <b>Hadoop</b>. 

The whole <b>Hadoop</b> framework is interesting as it is one of the main cornerstone of many tools: we shall see some of them in the following lessons: Hive, Pig, Spark, HBase, ...

<h2>Hive</h2>
<h3>Introduction</h3>

Hive is a very interesting element of the Data Engineer toolbox. It provides a SQL-like interface for tabular data. Development started in 2007 at Facebook and it was first released as an Open Source project in 2015. Today, Hive is managed by the Apache Foundation. 

<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Hive.png"/></center>


Hive is an abstraction of a relational database management system (RDBMS) but relies on Hadoop for distribution (repartition, partition, replication, …) and computation. It uses HQL (for Hive Query Language) which is a SQL like query language. 

Data is stored as flat files in HDFS and queries are actually translated from HQL to MapReduce jobs using YARN.

<h3> A word on execution planning</h3>

There is an important component of Hive, the metastore. The metastore is a local relational database used to store metadata on the location of the flat files in HDFS and the schema of the tables. 

The other important components of Hive are: 


md5-b8462210dfa823d1e5fedc2728095a52



The mechanics are shown in the following figure: 

@slider hive_execution_slider 


<div class="alert-info">
<span class="glyphicon glyphicon-info-sign"> <b>DAGs</b> (Directed Acyclic Graphs) are a very important concept of execution planning. A DAG is a graph whose nodes are individual tasks and edges represent dependencies between those tasks. The goal of a DAG is to be optimized: tasks that can be performed at once are regrouped and independent tasks are set to be run in parallel. 
</span></div>
<h3> Partitioning </h3>

As stated before, Hive is not really a <b>RDBMS</b>: it acts like it but it is truly HDFS that is storing the files. In fact, <b>Hive</b> is just an interface that allows to make SQL-like queries on very large datasets: the ability to distribute files on several machines, hence the larger capacities, is what make Hive more interesting than a regular RDBMS. 

But some queries may take a while to perform because of the size of the datasets... In order to speed up queries, Hive has been implemented with the ability to partition data on different columns. For example, if you imagine a dataset with clients. We could partition it on the nationality of our clients. This means that we will have one file for each nationality. This file can be partitioned but each partition will contain one and only one nationality. This means that queries based on nationality will be highly sped up as there are no need to check every record. 

You can have multiple levels of partitioning (nationality, age group, gender, ...). 

<h3>Installation</h3>

In this part, we will install Hive. We will have to configure Hive and install an external relational database to serve as Metastore. There are multiple available choices but we will use <a href="https://db.apache.org/derby/">Apache Derby</a>.

First we need to decompress the archive files (to ease the use of these folders we will rename them): 



md5-96f075b807a95f65b1928c91783e4695




Then we need to update the paths in the <code>~/.bashrc</code> file:



md5-c7d2c7807fc5b5fa11dea8a75befcc86




In this file we will paste the following lines:



md5-a9f28851f0886dcb5d65fb5aa2d9cc8c



Save the changes, close the file and commit the changes to the system with <code>source ~/.bashrc</code>.

Then we need to create folders in HDFS where data will be stored and change their rights (within HDFS) to give writing priviledges to group user.



md5-42ddb479ef8b4067ab4f821a8f063a95



There are some configuration files to change too: copy the <code>$HIVE_HOME/conf/hive-env.sh.template</code> file and change its name. 



md5-3633c34a9d750bd0ead9add0d3f93b03



Add the line: <code>export HADOOP_HOME=/home/ubuntu/hadoop</code>, save and close the file. We need also to change the file containing the whole configuration of Hive:



md5-50d2fa4e1246578e8d553323dfaf6669



This file contains a lot of information, a lot of parameters... To navigate through this file, you can use <code>ctrl + w</code> to look for a particular term. 

Here we need to make the following changes


md5-891258f39614a15b88f7d362c72e786a



We need to create a last file for Hive: 



md5-865416cb2903ef367c890fb926372206



In this file we need to paste the following lines: 



md5-93cf4b9652b2ff5f8beb2d57fd4ae0ee



We also need a directory for derby:


md5-8d39c34b6ca4bce7f676f35242b69700



Finally, the last step of the installation the initialization of the derby database:



md5-7826819d37efc6f0c7bf79042515ef12




Check that the file is indeed uploaded:


md5-7f706d861bb9b47d50fda0cdb5ab1a37



Next, open Hive CLI with <code>hive</code>. As with a lot of relational databases, you can use <code>SHOW DATABASES;</code> to see what databases are already in Hive. We will need to use one of them to carry on. Here we will of course need to create one:



md5-1d294095ae7344cba1d80a8bbf079e38



We should see what the tables in <code>my_hive_db</code> are with <code>SHOW TABLES;</code>: there are no tables yet. 

To create a table the syntax is very similar to regular RDBMS. Here we want to create a table <code>ratings</code>.



md5-fca00cddf277d9ae714306ef045e5a16



Note that we need to specify some things that are unusual at this step: how data is and will be stored. 

To load data into the table, the syntax is similar to regular RDBMS syntax: 



md5-b0cbf825bae7313fc2eebf7428e74c22



Notice that the path to the file is the HDFS path. If we want to load a file from the local file system, we can use the following tweak: <code>LOAD DATA LOCAL INPATH ... </code>. 

Once you have loaded the file, quit Hive with the <code>exit</code> command. 

To illustrate how Hive works we are going to list the files at the root of HDFS: 



md5-7f706d861bb9b47d50fda0cdb5ab1a37



The file <code>ratings.csv</code> has disappeared. In fact, it has been moved into the <code>/user/hive/warehouse</code>. We can list recursively the content of this folder: 



md5-d66eba042fedd3ba58fac87015df8f34



You should see that a folder <code>db.my_hive_db</code> has been created. It contains a folder <code>ratings</code> that correspond to the table in question and its content is a file <code>part-00000</code>. If the file was larger we could have had multiple partitions. 


We can print the content of this file: 



md5-83c56b4df7899908aafd6cd4d1b3e517



The content of the file has not been changed. 

Now let's return to Hive CLI: <code>hive</code>.

We are going to create a table to illustrate the Hive partitioning abilities. To create a partitioned table, we need to specify on which variable we want to partition: 




md5-55c33f01db3a4395a510d65d2c61d58b




We want to partition the table on the <code>rating</code> column so we have to specify it in last position. To insert data into the table, we will use a query on the <code>ratings</code> table: 



md5-32388c1051879268a2d8804b9de090c2



The <code>rating</code> column is in last place because of the definition of the table <code>ratings_part</code>. Once these commands are launched into Hive CLI, return to the shell and launch the following command to inspect the content of HDFS:



md5-a4ec028f32a33a500fbd4412c40367b0



You can see that the folder <code>ratings_part</code> is subdivided in multiple subfolders corresponding to the different partitions of the column <code>rating</code>.

We will just try to see if there are differences between the two tables: connect to the Hive CLI and run the following command: 



md5-1c0a8c44e69729068323d52eb180a282



Once you have seen the time taken to run the query, run the following: 



md5-825506cf8ff103cff417074ff4d4dc43



You should see a difference between the two tables: in the first case, we have to check every line while in the second case, the ratings are already stored in different partitions, so the query takes less time.

We are not going to go deeper on Hive for the moment: this is an interesting tool to use the power and the accessibility of RDBMS in a distributed framework.

<h2>Pig</h2>

Apache Pig is a useful tool to perform data manipulation in a distributed environment. It has its own language: <b>Pig Latin</b> and abstracts MapReduce jobs. The idea behind Pig is to simplify the use of MapReduce: for example, the wordcount jobs take about 60 lines in <b>Java</b> while it takes  5 lines with Pig. 

<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Pig.png"/></center>


Moreover, Pig is using lazy evaluations and DAGs to optimize the queries: the queries are passed through a parser, an optimizer, a compiler and then an execution engine to run the queries. Pig can use different orchestrators (Yarn, Spark, Tez, ...) 

<h3>Installation</h3>

In this part, we are going to install Pig. 

First extract the archive: 



md5-d4177969535d3e5e27ede2561b5d01e0



We simply need to add paths to the Pig in the <code>~/.bashrc</code> file.



md5-d6b4bdee0928ad7ed93c9946c8493674



Next, we need to launch Hadoop Job History Server: 



md5-6d2a0c2e16e5ab9964742acb0d7f1c55



Now to check that Pig is working, just run <code>pig</code>. This should open Pig CLI <code>grunt</code>. To leave this interface, you can use <code>quit</code>.

<h3>Pig Latin</h3>

The basic data structure is called a relation in Pig. It is basically a list of named objects. Those objects are of the same type: this means that the structure needs a schema. To load data, we use the <code>LOAD</code> keyword as following:



md5-442dc09cc49485dc6b1eb8305b8cd448



Except if you have missed the right path to the file, there should not be a lot of information printed when this command is launched: it is because of the lazy evaluation.

Here the schema is introduced by the <code>AS</code> keyword: here we have a column of integers and two columns of strings. 

The keyword <code>USING</code> introduces the function used to load data. There are many different functions corresponding to different file types. We will see later how to load directly data from Hive with a different function. 

If we want to execute the operations completely of partly, we can use the following keywords: 


md5-cd4788aa250c62ee7ae0965111b24840



To perform map operations, we can use the <code>FOREACH</code> and <code>GENERATE</code> keywords. For example, if we want to create a relation with only the column <code>genre</code>, we can use: 



md5-95e272b05e8fb48316ed8d136ff9a737



This line of code creates a relation named <code>genres_list</code> with the following schema <code>(genres_of_the_movie: chararray)</code>.

We can also perform flatmap operations (eg generating multiple lines per line), using the <code>FLATTEN</code> keyword: 



md5-1d25ef1e1e5314cf6b68a738a57e4942



As a reminder, the content of the column <code>genres_of_the_movie</code> is a pipe (<code>|</code>) separated file. 

We can perform Group By operations with <code>GROUP</code>: 



md5-60337deef128aca766f4a0b2af2de507



If you illustrate the content of the relation, you will see that it is similar to a dictionary with the values being list of keys: for example, <code>{'comedy': ['comedy', 'comedy'], ...}</code>. 

To reduce this relation, we can use <code>FOREACH</code> once again: 



md5-6d226d413824f4a1a34f3f0464413630



Notice that in this example, we have done something very similar to the wordcount example.

There are also other keywords that can be used (that are very similar to <code>SQL</code>): 


md5-debc74d8ebe933da93c20e606877a245



One other interesting thing is the ability to use user defined functions (UDFs). UDFs are specified in a Java archive and must follow certain <a href="https://pig.apache.org/docs/latest/udf.html">rules</a>. To import a function, you can use: 



md5-d00586a298fd668b9f468e9a02840393



Pig is interesting because there are a lot of different pre-defined functions (maths, text, ...) and you can define your own functions. Moreover it can be linked easily to Hive. For this you need to launch <code>grunt</code> with the commande <code>pig -useHCatalog</code> and use the following syntax while loading data:



md5-3957bca8f5eeec65ee469f046e42f010



And to store data directly in Hive, we can use the <code>org.apache.hive.hcatalog.pig.HCatStorer()</code>.

In some aspects, Hive and Pig are quite similar. They both can be used for Data Manipulation but the first one can also store data as an abstracted relational database. This is the main difference but the two tools also do not have the syntax. Basically, Pig is more advanced in terms of customization but it is really up to the user of these tools. 

<h2>Sqoop</h2>

Finally, this last part will deal with Apache Sqoop: this tool can be used for ETL (Extract Transform Load) pipelines. Basically, Sqoop is used to transfer data from relational databases to HDFS (and the other way around). 

<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Sqoop.png"/></center>


This can be very interesting to use the power of distributed computing power on regular databases, or to store transformed data into a datalake based on Hadoop. 

<h3>Installation</h3>

First we must decompress the archive: 



md5-7ac850de7d26cb0b7ed1cb7c5f33627f



Open the file <code>~/.bashrc</code> and paste the following lines: 



md5-d13ff604ed39c14ea767866515090b4f



Save, close and commit (using <code>source ~/.bashrc</code>) this file. 

Next we need to change a few lines in the configuration file: 



md5-c2330a0189e73a3fb97d734031bc557b



Paste those lines: 


md5-6a19aa62a3fc8198a3b57a6472fbc483



Finally, there is a file that we need to put into the <code>$SQOOP_HOME/lib</code> folder: 



md5-b0cea133b297d4a3f17de735a4daa1ef


<h3>Practice Sqoop</h3>

The main features of Sqoop are pretty easy to use. On your machine, we have installed <b>MySQL</b>: it contains a database named <code>my_db</code> and a table called <code>movies</code>. 

To import data from MySQL to HDFS, the syntax is the following: 



md5-13860ec96f19c54cf0fc31c07a2f4fc5



We are using <code>sqoop import</code> to import data and the arguments are:


md5-c777c06abf7f72f4de0e03cf62cd9c62



Once you have run this command, you can check if the data has been imported into HDFS. 



md5-dc46b0464b4b31b4a677b6d1bacefee3



Instead of importing the whole table, you can also import only some of it using the argument <code>--query</code>: 



md5-38d8255a8234eaa7dec7be13d2bd5815



This is very useful for ETL pipelines. Notice that we need to specify a condition with <code>WHERE</code> and the condition has to contain <code>\$CONDITIONS</code>. 

Arguments for the export from HDFS to MySQL are approximately the same: 


md5-c05182a3f189b98319d171c1a0530cc8



One other feature that is interesting is the ability to import data directly into Hive: 



md5-e0cba023ad46dd31b7043da98cd1e6c1



The table does not need to be created as Sqoop will create it during this command. If we want to import only a query, use the <code>--query</code> argument as before and if you want to import all the tables, change <code>sqoop import</code> to <code>sqoop import-all-tables</code> and <code>hive-table</code> to <code>hive-database</code>.

<h1>Conclusion</h1>

<p>In this lecture, we have seen the advantages of distributed over classical architecture: security, computation and storage capacities enhanced, cost-efficient, ... Hadoop is the main framework in this domain as it is Open Source and that a lot of tools are based on its source code: Hive and Pig are useful to manipulate data, create datasets that can be used by Data Scientists and Data Analysts and provide interfaces that are more useful while Sqoop is used to do ETL pipelines. </p>