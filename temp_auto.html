<hr style="border-color:#75DFC1; width:80%;"/>
<center><h1>Automation and deployment</h1></center>
<hr style="border-color:#75DFC1; width:80%;"/>

<p>One of the major aspects of Data Science that is often under-estimated is the deployment and automation: <br>Let’s say you have a Machine Learning model that can predict if a visitor on your website will be likely to purchase an item if retargeted. The model has been trained on historical data, e.g. on fixed data, and you need to put the model into production. There are several issues that you have to take into account. First, you need to set a production environment that should not interfere with other production environments. Then, you need to get your predictions into a way that is accessible to other programs. Finally, you want your production server to tell you if there is something wrong going on. <br>Those issues are very important and have to be dealt with, whether by the Data Scientist but more often by the Data Engineer. </p>

<h2>APIs</h2>
<p>In this part, we will show some aspects and advantages of using APIs when doing Data Science. Then, we will learn how to use <code>Flask</code> a Python library that is used to create simple APIs.</p>
<h3>Introduction to APIs</h3>
<p>API litterally stands for <b>Application Programming Interface</b> and is defined as <i>a set of subroutine definitions, communication protocols, and tools for building software</i>.</p>
<p>This is very vague but you have to think of APIs as you do about a power sockets: the power socket is a set of rules: the device that you want to plug in has to have two pins that are spaced with a certain length, not more, not less, and that can handle a given voltage, a given current intensity and a given frequency. You do not know what is behind the socket: it might be a nuclear power plant, a coal power plant, solar panels or wind turbines. All you know is the specifications of the socket: the rules of the API. On the other hand, the socket does not care about what you are plugin in: it will deliver the power no matter the device.</p>
<p>An API is based on the same principle. For example, a web API that is supposed to provide data when asked will not care about the program that is asking for data as long as it does it the right way. On the other hand, the program does not need to know what is going on the background: data may be stored in a flat file, in a relationnal database or just created on demand but this does not change anything for the program. This is very useful especially if you want to update the back end of an applcation without adapting the clients: if your data is growing and you need to change the way it is being requested but not change the code that request the data in a production environment, you can use an API that will isolate the software.</p>
<p>This is the cornerstone of micro service architectures and some companies have made it a credo: Amazon aks its developper to create only features that are accessible through APIs and to provide the documentation for the rules of those APIs. This is a very interesting vision of development as developpers can easily use features implemented by other teams. Moreover, with a sound choice of credentials attribution, you can even commercialise your APIs or share it with partners.</p>
<p>There are a lot of different APIs: web APIs, code APIs, … In fact, if you think about a code library, it is an API: you do not really know what is inside the functions, the classes but you have the specifications about the required arguments and about the aim of those objects.</p>
<p>In this course, we will focus on Web APIs</p>
<h3>Web APIs and HTTP requests</h3>
<p>Web APIs are available through the internet and provide data interaction through HTTP requests. HTTP stands for <b>HyperText Transfer Protocol</b>. It is an Application Layer protocol (according to the OSI system) and is the standard for web communications.</p>
<p>An HTTP request is composed of multiple elements:
<ul>
<li><b>a method</b>: what to do.</li>
<li><b>a URI</b>(Universal Resource Identificator): where to go</li>
<li><b>a body and headers</b>: additional information </li>
</ul>
<p>
The available methods are:
</p>
<ul>
<li><code>GET</code>: query data</li>
<li><code>PUT</code>: update data</li>
<li><code>POST</code>: create data</li>
<li><code>DELETE</code>: delete data</li>
<li><code>...</code></li>
</ul>
<p>
    URIs (or URLs, the nuance is not clear) are basically defined by multiple parts :</p>
<ul><li>optionally the protocol: <code>http://</code> or <code>https://</code></li>
<li>optionally <code>www.</code> to indicate that the URI is over the web.</li>
    <li>a domain name that serves as a placeholder for the ip address of the machine(s) serving the URI: <code>example.com</code></li>
<li>an endpoint that refers to a given route of the server: <code>/</code> for example</li>
</ul>
<p>This gives us the URI <code>http://example.com/</code>
</p>
<p>
To make an HTTP request from the terminal, we can use the <code>curl</code> command. The syntax to make a request is the following:
</p>

<div class="sh code" id="code-9">
curl -X GET http://example.com
</div>

<p>
The argument <code>-X</code> introduces the method: here <code>GET</code>. Then we can put the URI. Here we have a website so the output is an HTML file but generally, with an API, we should get XML or JSON style file that fits better data queries.<br>
For example:
</p>

<div class="sh code" id="code-10">
curl -X GET https://jsonplaceholder.typicode.com/posts/1
</div>

<p>With this request, we receive a stringified JSON object containing information about a post. This domain name serves as a dummy API for developpers. You can see <a href="https://jsonplaceholder.typicode.com/">here</a> the specifications of the fake API. </p>
<p>Here we have requested the post with the ID <code>1</code>. We can get all the posts with a GET request at the <code>/posts</code> endpoint. This is a pattern we commonly see: the endpoint with an ID returns an individual observation while without any ID it returns all the observations.</p>
<p>The HTTP response is given with a body and a header. To see the headers, we can add the <code>-i</code> argument to the command.</p>

<div class="sh code" id="code-11">
curl -X GET -i https://jsonplaceholder.typicode.com/posts/1
</div>

<p>In the header, you can see some information about the content of the response, ... One important thing is the status code: 200.<br>
Status codes are used to provide information about the success of the request:
</p>
<ul>
<li><code>200</code>: means the request succeeded</li>
<li><code>40X</code>: means an error on the client side</li>
<li><code>50X</code>: means an error on the server side</li>
</ul>
<p>
To perform a <code>PUT</code> request on our URI, we can use the following command:
</p>

<div class="sh code" id="code-12">
curl -X PUT -i -H "Content-Type: application/json" -d '{"id": 1, "content": "hello world"}' https://jsonplaceholder.typicode.com/posts/1
</div>

<p>
The <code>-H</code> argument introduces the headers of the request. Here, we tell the system the shape/type of data that is sent. The <code>-d</code> argument introduces the body of the request. <br>
The headers of the response tell us if the request failed or succeeded.
</p>
<h3>Flask</h3>
<p><a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a> is a Python API that allows to build web APIs or websites. We can do a lot of things with Flask but here we will only build a simple API to serve data about users.</p>
<p>Users data is contained in a file named <code>~/data.json</code>. You can see the content of the file by using <code>cat ~/data.json</code>.
<p>We will provide the following functions:</p>
<table class="table">
<thead class="table-dark">
<tr><th>Endpoint</th><th>Method</th><th>Function</th></tr>
</thead>
<tr><td>/users</td><td>GET</td><td>returns all the users data</td></tr>
<tr><td>/users/{int:id}</td><td>GET</td><td>returns the data of the user with the specified id</td></tr>
<tr><td>/users/{int:id}</td><td>PUT</td><td>modify data of the user with the specified id</td></tr>
<tr><td>/users</td><td>POST</td><td>create a user with a new id and the given information</td></tr>
<tr><td>/users/{int:id}</td><td>DELETE</td><td>delete the user with the specified id</td></tr>
</table>

<p>To build the API, first you need to open a Python file with <code>nano</code>:</p>

<div class="sh code" id="code-13">
nano ~/api.py
</div>

<p>We need to implement the <code>Flask</code> class.</p>

<div class="python code" id="code-1">
from flask import Flask
api = Flask(import_name='my_api')
</div>

<p>We need to specify the <code>import_name</code> argument when instantiating the class.</p>
<p>We will load all the data at once and then perform changes on the data:</p>

<div class="python code" id="code-2">
import json
with open('./data.json', 'r') as file:
    data = json.load(file)
</div>


<p>Once the class is instantiated, we can indicate give it the functions to perform using the decorator <code>@api.route</code>. </p>

<div class="python code" id="code-3">
from flask import jsonify
@api.route('/users', methods=['GET'])
def return_users():
    return jsonify(data)
</div>

<p>The decorator is a special feature of Python: a decorator is used to wrap the function defined right under it. Here it is used to tell what function should be called when the URI is requested with the right method. The decorator takes the name of the endpoint and also the methods.<br>
The <code>jsonify</code> method is used to turn data into a string that looks like a json.</p>
<p>We just need to add the line to run the app: </p>

<div class="python code" id="code-4">
if __name__ == '__main__':
    api.run()
</div>

<p>Now save and leave the file. Launch it with: </p>


<div class="sh code" id="code-14">
python3 ~/api.py
</div>


<p>By default, the API is running at the IP <code>http://localhost:5000/users</code>. You can request the data with: </p>


<div class="sh code" id="code-15">
curl -X GET -i http://localhost:5000/users
</div>


<p>So our first endpoint is working. Now reopen the file, we will add the other endpoints. Add those lines before the <code>main</code> part.</p>

<div class="python code" id="code-5">
from flask import abort

@api.route('/users/<user_id>', methods=['GET'])
def return_user(user_id):
    for user in data:
        if user['id'] == int(user_id):
            return jsonify(user)
    # in case the user is not found
    abort(404)
</div>


<p>Here we added certain elements that are interesting: first the <code>user_id</code>. It is present in the endpoint and is the only argument of the function. Moreover, if the specified ID is not in our data, we must send an error code. 404 is generally the standard status code for a resource not found.<br>
Leave the file and relaunch the API.<br>
Try the following commands</p>

<div class="sh code" id="code-16">
curl -X GET http://localhost:2222/users/1
curl -X GET http://localhost:2222/users/1000
</div>


<p>The second one should throw a 404 error.<br>
We can personalize the error code with the following function</p>


<div class="python code" id="code-6">
from flask import make_response

@api.errorhandler(404)
def resource_not_found(error):
    return make_response(jsonify({'error': 'Resource not found'}), 404)
</div>


<p>Note that we can personalize even more the messages by using a 410 error code into <code>abort</code> and throw a 404 error code in the <code>errorhandler</code> of the code 410.</p>
<p>Let's reopen the file and add the last endpoints</p>



<div class="python code" id="code-7">
@api.errorhandler(400)
def bad_request(error):
    return make_response(jsonify({'error': 'Bad request'}), 400)


from flask import request
@api.route('/users/<user_id>', methods=['PUT'])
def update_user(user_id):
    user = None
    for u in data:
        if u['id'] == int(user_id)
            user = u
            data.remove(u)
            break
    if not user:
        abort(404)
    if 'name' in request.json or 'age' in request.json:
        if 'name' in request.json:
            user['name'] = request.json['name']
        if 'age' in request.json:
            user['age'] = request.json['age']
    else:
        abort(401)
    data.append(user)
    return jsonify(user)

@api.route('/users', methods=['POST'])
def create_user():
    new_id = max(data, key=lambda u: u.get('id')) + 1
    new_user = {'id': new_id}
    if 'name' in request.json:
        new_user['name'] = request.json['name']
    if 'age' in request.json:
        new_user['age'] = request.json['age']
    data.append(new_user)
    return jsonify(new_user)

@api.route('/users/<user_id>', methods=['DELETE'])
def delete_user(user_id):
    for u in data:
        if u['id'] == int(user_id):
            data.remove(u)
            return jsonify(u)
    abort(404)
</div>


<p>You can try the following requests to test your API:</p>


<div class="sh code" id="code-17">
# changing the name of the user with id 1
curl -X PUT -i -H "application/json" -d "{'name': 'charles'}" http://localhost:5000/users/1
# changing the job of the user with id 1
# should throw an error
curl -X PUT -i -H "application/json" -d "{'job': 'data scientist'}" http://localhost:5000/users/1
# changing the name of the user with id 1000
# should throw an error
curl -X PUT -i -H "application/json" -d "{'name': 'charles'}" http://localhost:5000/users/1000
# adding a new user
curl -X POST -i -H "application/json" -d "{'name': 'daniel'}" http://localhost:5000/users
# deleting the user with ID 42
curl -X DELETE -i http://localhost:5000/users/42
</div>


<p>Here we have used an API to interact with data but we can also perform Machine Learning task provided data.</p>
<h3>Exercise</h3>
<p>Create a very simple API that will send a sentiment analysis score using only a GET method. The endpoint should be <code>/sentiment/<sentence></code>. There are multiple ways to perform this task but the simplest is to use <code>VaderSentiment</code>.</p>

<h2>Virtualization with Docker</h2>
<h3>Introduction to virtualization</h3>
<p>Virtualization is the fact of mimicking parts of a machine in order to isolate a process from the rest of the machine. Isolating processes on a server is very important: it avoids version conflicts. Moreover, it allows to have a development environment which is exactly the same as the production environment.</p>
<p>There are a lot of ways to perform virtualization:
<ul>
<li><b>Virtual machines</b>: virtual machines contain everything that makes a computer (OS, dedicated storage, ...). It is exactly like having a computer inside another computer.</li>
<li><b>Containers</b>: containers are very similar to virtual machines except that they rely on the OS of the host machine.</li>
<li><b>Virtual environments</b>: virtualization technique proper to Python that allows to isolate versions of libraries for different processes.</li>
</ul>
<p>In this course we will not cover virtual machines even though there is a lot of things to learn about <b>AWS</b>, <b>GCP</b>, <b>Azure</b> and other cloud providers.<br>
We will cover quickly the virtual environments and containers.</p>
<h3>Virtual environments</h3>
<p>Virtual environments are the Python response to the isolation issue. Let's imagine that you have a process running on a server. This process has been coded with the <code>1.4.5</code> of a library. Later you have developed another process using a modern version of the library: <code>2.0.0</code>. To have both processes running on the same server, you need to have two version of the same library: this is not possible without virtual environments.</p>
<p>To create a virtual environment, you can use the following command.</p>


<div class="sh code" id="code-18">
python3 -m venv my_virtual_environment
</div>


<p>Once this is done, you should see that a folder <code>my_virtual_environment</code> has been created.  To activate the virtual environment, you can use:</p>


<div class="sh code" id="code-19">
source ./my_virtual_environment/bin/activate
</div>


<p>You should see a <code>(my_virtual_environment)</code> indicator that tells us that the environment is activated. Now you can install any library, with the specified versions using <code>pip3</code>. </p>
<p class="alert-info">When you create a python project, you should always create a virtual environment. You should create a file named <code>requirements.txt</code> in which you will put the name of the libraries used in your project. You can easily install libraries in a text file by using <code>pip3 install -r requirements.txt</code>. Once the project is done, to fix the libraries, you should use <code>pip3 freeze > requirements.txt</code> to get the exact versions of the libraries.</p>
<p>To deactivate the virtual environment, you can type in <code>deactivate</code>.</p>
<h3>Docker</h3>
<h4>Containerization</h4>
<p>Containerization is the next level of virtualization: we do not only encapsulate libraries, we can encapsulate complete applications. Containers are different from virtual machines as they do not encapsulate the OS. </p>
<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Docker.png" style="width:50%"></center>
<p>Docker is a technology of containerization that is used a lot today: it is light, handy and a lot of companies have developed their own containers and made them available through the <a href="https://hub.docker.com/search?q=&type=image">DockerHub</a>.</p>
<p>Docker is very simple to use with a Linux system and implements a lot of interesting features: networks, volumes, ...</p>
<h4>Launching a container</h4>
<p>In this part, we will see a to launch the Docker daemon.<i>The installation of Docker is a bit long and not very interesting so it has already been done.</i><br>
To launch the daemon, type in the following line:</p>

<div class="sh code" id="code-20">
sudo service docker start
</div>

<p>We need to download an <b>image</b> of a Docker before running it: an image is not really a container. It is a pattern for containers: you can think of Docker images as classes and containers as class instances. To do so, we can use the command:</p>

<div class="sh code" id="code-21">
docker pull hello-world:latest
</div>

<p><code>hello-world</code> is a dummy image to test Docker that just prints a message and shuts down the container. <br>
Sometimes, different versions of an image are available: The <code>:</code> introduces the tag, e.g. the name of the version.<br>
To launch this container, we can do :</p>

<div class="sh code" id="code-22">
docker container run hello-world:latest
</div>

<p>If you have not run the <code>docker pull</code> before the <code>docker container run</code> command, the pulling is done before launching the container.<br>For example, run the following command to start a Ubuntu image container.</p>

<div class="sh code" id="code-23">
docker container run ubuntu:latest
</div>

<p>This is not a very interesting command as it shuts down immediately. But we will see how to keep the container up and how to interact with it. We can use the <code>-it</code> flag and add the entry point, e.g. the way to enter the container:</p>

<div class="sh code" id="code-24">
docker container run -it ubuntu:latest bash
</div>

<p>You can check that the container looks indeed like a Ubuntu system. You can exit it at any time using <code>exit</code>.</p>
<p>You can list all the containers that are running by using the <code>docker container ls</code>. Here we have none but we can see the history of the dockers that were run by adding the <code>-a</code> flag.</p>

<div class="sh code" id="code-25">
docker container ls -a
</div>

<p>We have seen how to list the containers but we can also list the images by using.</p>

<div class="sh code" id="code-26">
docker image ls
</div>

<p>Docker containers are given a name but you can name them with the <code>--name</code> flag:</p>

<div class="sh code" id="code-27">
docker container run --name my_hello_world_container hello-world:latest
</div>

<p>We can restart a stopped container by using its name:</p>

<div class="sh code" id="code-28">
docker container start -a my_hello_world_container
</div>

<p>Without the <code>-a</code> flag, the container is run in detached mode.<br>Note also that  a container cannot be named with the same name as another container, up or stopped.</p>
<p>We can remove containers that are stopped by using the command <code>docker container rm</code>: </p>

<div class="sh code" id="code-29">
docker container rm my_hello_world_container
</div>

<p>If you run <code>docker container ls -a</code>, this container is not present anymore.<br>We can perform this removing task automatically by using the <code>--rm</code> flag:</p>

<div class="sh code" id="code-30">
docker container run --name my_second_hello_world_container --rm hello-world:latest
</div>

<p>If you want to run a container in detached mode, e.g. in background, you can use the <code>-d</code> flag: </p>

<div class="sh code" id="code-31">
docker container run -it -d --name my_ubuntu_container ubuntu:latest bash
</div>

<p>You should see it running in the command <code>docker container ls</code>. You can shut it down by using:</p>

<div class="sh code" id="code-32">
docker container stop my_ubuntu_container
docker container ls
</div>

<h4>Port forwarding</h4>
<p>In this part, we will use the <b>ElasticSearch</b> image. ElasticSearch is a search engine that allows to look through large amounts of data very quickly. Interactions can be done through an API running at the 9200 port of the serving machine via HTTP requests. Pull the image corresponding to the <code>7.2.0</code> tag.</p>

<div class="sh code" id="code-33">
docker pull elasticsearch:7.2.0
</div>

<p>To run it, you can use the following command: </p>

<div class="sh code" id="code-34">
docker container run -d --rm -e "discovery.type=single-node" --name my_es_container elasticsearch:7.2.0
</div>

<p>
The <code>-e</code> flag allows us to define an environment variable inside the container. <br>Here we define the <code>discovery.type</code> variable with the value <code>single-type</code>.
</p>
<p>We want the API to be accessible but the following command will throw an error:</p>

<div class="sh code" id="code-35">
curl -X GET -i http://localhost:9200
</div>

<p>This is because the IP of the container is not the IP of the machine serving it. To check on which IP the container is running, we can use the <code>docker inspect</code> command.</p>

<div class="sh code" id="code-36">
docker inspect my_es_container
</div>

<p>The line that we want to see, for now, is the line containing the <code>IPAddress</code> argument: </p>

<div class="sh code" id="code-37">
docker container inspect my_es_container | grep IPAddress
</div>

<p>The IP should be <code>172.17.0.2</code> or something similar. We can use this IP Address to run our request (change the IP if you do not have the same as us):</p>

<div class="sh code" id="code-38">
curl -X GET -i http://172.17.0.2:9200
</div>

<p>This is a basic request in ElasticSearch that returns information about the ElasticSearch cluster. There, we can see that the name of the cluster is <code>docker-cluster</code>. </p>
<p>This is great but it can be a bit tedious to worry about networking. In fact, Docker allows to forward directly the ports of the container to the port of our machine.<br>First, stop the ElasticSearch container:</p>

<div class="sh code" id="code-39">
docker container stop my_es_container
</div>

<p>Thanks to the <code>--rm</code> flag, it has been removed so we can start a new container with the same name. <br>Here we are going to use the <code>-p</code> flag to indicate the port forwarding addresses.</p>

<div class="sh code" id="code-40">
docker container run -d -e "discovery.type=single-node" -p 9200:9200 -p 9300:9300 --name my_es_container elasticsearch:7.2.0
</div>

<p>The <code>-p</code> flag takes the port of the container to be forwarded, the <code>:</code> separator and the port of the host machine receiving the flow.<br>
We can check that this is working by using the command:</p>

<div class="sh code" id="code-41">
curl -X GET -i http://localhost:9200
</div>

<p>We will explore the networking aspects of Docker more in the next parts<br>For now you can stop the container.</p>

<div class="sh code" id="code-42">
docker container stop my_es_container
</div>

<h4>Volumes</h4>
<p>One thing that you may not have noticed yet is that Docker containers do not persist changes. For example, if you launch a Ubuntu container:</p>

<div class="sh code" id="code-43">
docker container run --name my_ubuntu_container -it ubuntu:latest bash
</div>

<p>In this container, we are going to create a file and leave the container:</p>

<div class="sh code" id="code-44">
echo "hello world from Docker" > /home/test.txt
cat /home/test.txt
exit
</div>

<p>You can restart the container using the <code>docker start --interactive -a my_ubuntu_docker</code> command. <br>Check that the file is still there:</p>

<div class="sh code" id="code-45">
cat /home/test.txt
</div>

<p>This is great but we may want to delete the docker container. Exit the container with <code>exit</code>, remove it with and relaunch a similar container:</p>

<div class="sh code" id="code-46">
docker container rm my_ubuntu_container
docker container run --name my_ubuntu_container -it ubuntu:latest bash
</div>

<p>If you try to print the content of our file, it has disappeared:</p>

<div class="sh code" id="code-47">
cat /home/test.txt
</div>

<p>... should throw an error: <code>cat: /home/test.txt: No such file or directory</code>.<br>This is because changes are not persisted. To persist changes onto the disk we can use volumes.<br>To create a volume, you can use:</p>

<div class="sh code" id="code-48">
docker volume create my_volume_for_ubuntu
</div>

<p>You can list all existing volume by using:</p>

<div class="sh code" id="code-49">
dockre volume ls
</div>

<p> ... and inspect it with :</p>

<div class="sh code" id="code-50">
docker volume inspect my_volume_for_ubuntu
</div>

<p>You can see that a Docker volume is in fact a local repertory located at <code>/var/lib/docker/volumes/my_volume_for_ubuntu/_data</code>. <br>To mount this volume to a container, we can use the <code>--mount</code> argument. It takes multiple information separated by commas.
</p>
<ul><li><code>type=volume</code>: the type of mounting.</li>
<li><code>src=my_volume_for_ubuntu</code>: the Docker volume to use.</li>
<li><code>dst=/home</code> the absolute path of the directory where it is going to be mounted.</li>
</ul>
<p>
We just have to delete the existing container and launch a new one with this new argument.</p>

<div class="sh code" id="code-51">
docker container rm my_ubuntu_container
docker container run -it --name my_ubuntu_container --mount type=volume,src=my_volume_for_ubuntu,dst=/home --rm ubuntu:latest bash
</div>

<p>Once the container is launched, we repeat the command:</p>

<div class="sh code" id="code-52">
echo "hello world from Docker" > /home/test.txt
</div>

<p>In another console, you can see the content of the mounted directory by using the command:</p>

<div class="sh code" id="code-53">
 sudo cat /var/lib/docker/volumes/my_volume_for_ubuntu/_data/test.txt
</div>

<p>You should see the <code>test.txt</code> file. You can also do this the other way round.<br>On the second terminal, type in the following command.</p>

<div class="sh code" id="code-54">
echo "hello world from Ubuntu" > ~/test2.txt
sudo mv ~/test2.txt /var/lib/docker/volumes/my_volume_for_ubuntu/_data/test2.txt
</div>

<p>You can return on the container console and run <code>cat /home/test2.txt</code>: the two folders are indeed one and the same.</p>
<p>Now shut down the container: you can use <code>exit</code> inside the container or in the other terminal use the <code>docker container stop my_ubuntu_container</code> command.<br> Now launch a new container with the same volume parameters:</p>

<div class="sh code" id="code-55">
docker container run -it --name my_ubuntu_container --mount type=volume,src=my_volume_for_ubuntu,dst=/home --rm ubuntu:latest bash
</div>

<p>Run the command <code>ls /home/</code>: the files are still present. You can also open a third terminal and launch another container mounted on the same volume:</p>

<div class="sh code" id="code-56">
docker container run -it --name my_ubuntu_container2 --mount type=volume,src=my_volume_for_ubuntu,dst=/home --rm ubuntu:latest bash
</div>

<p>Run the command <code>ls /home/</code>: the files are also present.<br>You can shutdown both containers.</p>
<p>Finally, we can also delete the volume by using</p>

<div class="sh code" id="code-57">
docker volume rm my_volume_for_ubuntu
</div>

<h4>Networks</h4>
<p>Networks are used to make different dockers work together. This allows communication to go directly from a container to another without passing by the host machine.<br>To list the networks, you can use: </p>

<div class="sh code" id="code-58">
docker network ls
</div>

<p>By default there are only two networks: <code>host</code> and <code>bridge</code>. <br>In one terminal, launch an ElasticSearch container:</p>

<div class="sh code" id="code-59">
docker container rm my_es_container
docker container run -d --rm -e "discovery.type=single-node" --name my_es_container elasticsearch:7.2.0
</div>

<p>In another terminal inspect the container:</p>

<div class="sh code" id="code-60">
docker container inspect my_es_container
</div>

<p>One of the last attribute of the container is <code>Networks</code>. You can see here that by default it is running on the <code>bridge</code> network. If you do not specify the network, it is this network that will be chosen.<br>
You can also see which containers are running on a network if you do:</p>

<div class="sh code" id="code-61">
docker network inspect bridge
</div>

<p>Under the attribute <code>Containers</code>, you can see the containers that are running on this network.<br>Now stop the container and start another one on the <code>host</code> network by adding the <code>--network</code> argument:</p>

<div class="sh code" id="code-62">
docker container stop my_
docker container run -d --rm -e "discovery.type=single-node" --network host --name my_es_container elasticsearch:7.2.0
</div>

<p>The host network is actually the IP of the host machine that is used. You can see this by doing the HTTP request to get information on the ElasticSearch cluster:</p>

<div class="sh code" id="code-63">
curl -X GET localhost:9200
</div>

<p>The <code>host</code> network is useful when you do not want to worry about networking at all. Yet the idea behind Docker networks is to isolate communications between  containers.<br>If a process requires multiple containers to run, you want them to be able to communicate between them without interference from other containers. </p>
<p>To create a network, we can use the following command:</p>

<div class="sh code" id="code-64">
docker network create my_network
</div>

<p>List the networks with <code>docker network ls</code> and your new network should appear. Now we can use the <code>--network</code> argument to link a container to this network:</p>

<div class="sh code" id="code-65">
docker container stop my_es_container
docker container run -d --rm -e "discovery.type=single-node" --network my_network --name my_es_container elasticsearch:7.2.0
</div>

<p>To run HTTP queries on the network, we are going to use the <code>appropriate/curl:latest</code> image. First, we need to know the IP address of the container:</p>

<div class="sh code" id="code-66">
docker container inspect my_es_container | grep IPAddress
</div>

<p>The address should be something like: <code>172.18.0.2</code>. Note that the second figure has been incremented by 1 as this represents a new network. Previously, when the network was the default one, e.g. the bridge network, it was <code>172.17.0.2</code>. If you had containers to this network it is the last number that is incremented. <br>To run the HTTP request, you can use the following command (if the IP is not the same, change it):</p>

<div class="sh code" id="code-67">
docker container run --network my_network appropriate/curl -fsSL http://172.18.0.2:9200
</div>

<p>You should get the same result as before. An interesting feature is that if you want to avoid looking for the actual address of the container, you can refer to it via its name:</p>

<div class="sh code" id="code-68">
docker container run --network my_network appropriate/curl -fsSL http://my_es_container:9200
</div>

<p>This should work as well.</p>
<p>Finally you can delete the network by using <code>docker network rm my_network</code> but you have to make sure that the containers hosted by this network are deleted.</p>
<h4>DockerFile</h4>
<p>So far, we have seen how to deal with pre-existing images but you can create your own images thanks to the <b>DockerFile</b>.  <br>Working with pre-existing images is great because you can very easily deploy complex tools (for those who have done the course on Hadoop, there are images containing ready Hadoop installations), in a single line and to interact with it very easily. You can of course simulate the use of those or as easily use the Docker containers in the production environment. But when you need to make your application work in the production environment as well as in your home environment, you can use Docker to encapsulate your application: the isolation from the host machine will guarantee that the application will work.</p>
<p>DockerFile are in facts simple text files containing the blue print of the image.</p>
<p>We will encapsulate our Flask API from before. First we need to open the DockerFile with <code>nano</code></p>

<div class="sh code" id="code-69">
nano ~/dockerfiles/DockerFile
</div>

<p>A lot of Docker images inherit from another Docker image. To get a very basic set up, you can use the Debian image. In the DockerFile, the first line should be:</p>

<div class="dockerfile code" id="code-80">
FROM debian:latest
</div>

<p>Then we can execute bash commands inside the image using <code>RUN</code>: </p>

<div class="sh code" id="code-70">
RUN apt-get update && apt-get install python3-pip -y && pip3 install flask==1.1.1
</div>

<p>Using <code>&&</code> allows us to speed up the processes: multiple commands are executed at once and this lightens the image.<br>Next, we need to add files to the image: the Python script and the data file. To do so, we can use the <code>ADD</code> keyword:</p>

<div class="dockerfile code" id="code-81">
ADD /home/ubuntu/api.py /my_app_folder/api.py
ADD /home/ubuntu/data.json /my_app_folder/data.json
</div>

<p>In fact, <code>ADD</code> works a lot like the <code>cp</code> in Bash except that we are taking files from the local file system and copy them to the image.</p>
<p>Next, to launch the app, we need to change present working directory. The <code>WORKDIR</code> keyword is used a bit as <code>cd</code>:</p>

<div class="dockerfile code" id="code-82">
WORKDIR /my_app_folder/
</div>

<p>Next, we have seen that our application is running on the port 5000 of the host machine, so we need to expose the port 5000: this will allow us to do port forwarding or to request the container by going on the same network:</p>

<div class="dockerfile code" id="code-83">
EXPOSE 5000
</div>

<p>Finally, we need a command to launch when the container is launched:</p>

<div class="dockerfile code" id="code-84">
CMD python3 api.py
</div>

<p>You can now leave the file after saving it: we are ready to build the image. To build the image from a DockerFile, we can do the following:</p>

<div class="sh code" id="code-71">
docker image build /home/ubuntu/dockerfiles/ -t my_image_for_flask
</div>

<p>You can see that we do not indicate a file but a context, e.g. a folder containing the DockerFile. This means that if you need to refer to certain files in the DockerFile and you want to use the relative path, it has to be from this folder. <br>The <code>-t</code> argument is used to name and optionally add a tag to your image. By default, the tag is <code>:latest</code>. <br>Now that our build is successful, we can run launch our own image:</p>

<div class="sh code" id="code-72">
docker container run -d --name my_own_container -p 5000:5000 my_image_for_flask:latest
</div>

<p>You can now run the command we have used before to get all the users:</p>

<div class="sh code" id="code-73">
curl -X GET http://localhost:5000/users
</div>

<p>Now that we have our image, you can see how easily we can take this image and use this image into a production server where you only need Docker to get a whole application running. <br>To export an image and send it to the production server (without going through a git repo or through DockerHub), we can use :</p>

<div class="sh code" id="code-74">
docker image save --output ~/my_image_for_flask.tar my_image_for_flask
</div>

<p>To go from the tar file to the image, we can do the reverse thing:</p>

<div class="sh code" id="code-75">
docker image load -i ~/my_image_for_flask.tar my_image_for_flask
</div>

<p>We will just end this part by doing a quick recap of the main commands used in a DockerFile:</p>
<table class="table">
<thead class="table-dark">
<tr><th>Command</th><th>Function</th></tr>
</thead>
<tr><td><code>FROM</code> image_name:tag</td><td>inherit the image from another image</td></tr>
<tr><td><code>RUN</code> bash_ command></td><td>run bash commands</td></tr>
<tr><td><code>ADD</code> path_to_local_file path_to_file_in_the_image</td><td>copy file from the local file system to the image</td></tr>
<tr><td><code>WORKDIR</code> path_in_the_image</td><td>change the current working directory</td></tr>
<tr><td><code>EXPOSE</code> port </td><td>expose a port</td></tr>
<tr><td><code>CMD</code></td><td>command launched when the container is started</td></tr>
<tr><td><code>ENTRYPOINT bash_command</code></td><td>define the interactive modes</td></tr>
<tr><td><code>ENV variable value</code></td><td>define environment variables inside the image</td></tr>
<tr><td><code>VOLUME</code> [mounting_point]</td><td>mount points to be created</td></tr>
<tr><td><code>LABEL</code></td><td>add metadata to the image</td></tr>
</table>
<p>There are other commands that can be used but these are the main ones. For more information, you can go to the <a href="https://docs.docker.com/engine/reference/builder/">reference</a>.</p>
<h4>Docker Compose</h4>
<p>In this last part on Docker, we will rapidly cover Docker compose: Docker compose is a tool that allows to deploy several containers at the same time with given settings.<br>
In a file named <code>~/docker-compose.yml</code>, paste the following lines:</p>

<div class="yml code" id="code-79">
version: '3'
services:
  jupyter:
    image: jupyter/minimal-notebook
    container_name: my_jupyter_from_compose
    networks:
      - bridge
    ports:
      - "8888:8888"
    environment:
      JUPYTER_TOKEN: 'password'
  elasticsearch:
    image: elasticsearch:7.2.0
    container_name: my_es_from_compose
    networks:
      - bridge
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      discovery.type: single-node
networks:
  bridge:
</div>

<p>If we take a look at the code in this file, we first have to specify the version of docker-compose that we will be using. Then the argument <code>services</code> introduces the containers. Our first container is an instance from the <code>jupyter/minimal-notebook</code>. <br>The name of this container will be <code>my_jupyter_from_compose</code>. It will use the network <code>ubuntu_my_network_from_compose</code> and forward the port 8888 of the container to the port 4444 of the host machine. Finally, we define an environment variable: <code>JUPYTER_TOKEN</code> is set to <code>password</code>. </p>
<p>Information for the second container is very similar except for the ports: the syntax is much simpler. It is a simple ElasticSearch container.<br>
Finally, the last part is used to give the networks that will be used.<br>
You can leave the file and launch the following command to start both containers:</p>

<div class="sh code" id="code-76">
docker-compose up
</div>

<p>Open a new terminal and check the running containers.</p>

<div class="sh code" id="code-77">
docker container ls
</div>

<p>You can also inspect the network:</p>

<div class="sh code" id="code-78">
docker network inspect ubuntu_my_network_from_compose
</div>

<p>Now open the port of your machine at port 4444: you should see a Jupyter notebook invite. You can enter Jupyter with the password <code>password</code>.  Then open a new notebook, paste the following Python code in a cell and run it: </p>


<div class="python code" id="code-8">
import requests
import pprint
import json

content = json.loads(requests.get('http://my_es_from_compose:9200').content)

pprint.pprint(content)
</div>

<p>By printing the result you can see that we are indeed connected to the same network as our ElasticSearch container.<br>
Docker-compose is a useful tool to deploy easily a complex infrastructure of multiple containers. There are a lot of other <a href="https://docs.docker.com/compose/compose-file/">parameters</a> but this is not the aim of this course.</p>
<p>Docker is a very interesting tool that allow us to deploy very easily complex apps and isolate their environment from the host machine which makes deployment very easy.</p>
