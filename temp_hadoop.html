<head>
	<style>
		table {
			margin: 20px;
		}

		body {
			font-family: Source Sans Pro,sans-serif;
		}
		.slider {
			width:80%;
			margin-left: auto;
			margin-right: auto;
			height:auto;
		}

		.code {
			margin: 20px;
			border-radius: 5px;
			border-color: #000000;
		}
		.alert-info {
		    margin: 20px;
			border-radius: 5px;

		}
	</style>
</head>
<body>
<div class="container">
<hr style="border-color:#75DFC1; width:80%;"/><center>
<h1>Distributed Architectures, Hadoop and tools</h1></center>
<hr style="border-color:#75DFC1; width:80%;"/>
<p>
The volume of data created is exponentially increasing: to get a sense of this volume, each minute 103 million spam emails, 3.6 million Google searches, 450 000 tweets are exchanged. The forecast are that this volume will be multiplied by 10 in 2025. 
</p>
<p>
In order to draw value from those data, we need to be able to store and exploit them. 
</p>

<h2>Distributed Architectures vs Classical Architecture</h2>
<p>
When faced to the problem of Big Data storage, there are mainly two ways to increase one's storing capacities or computing power:
</p>
<ul>
<li> <b>Scaling Up</b>: buying a more powerful machine</li>
<li> <b>Scaling Out</b>: buying less powerful machines and make them work together</li>
</ul>
<h3>Some definitions </h3>
<p>
A <b>cluster</b> is a set of machines also called <b>nodes</b>. Those machine can be linked to each other or have special network architectures. 
Generally speaking some machines are more important than other: they are more powerful and are thus more oriented toward orchestration, planning, ... <br/>Those are called <b>master</b> nodes while the regular one are called <b>worker</b> or <b>slave</b>: they are the one actually doing the computations and the storing.
</p>
<h3>Distributed architectures are cheaper</h3>
<p>
For decades, improvements on transistor capacities, hence on storage and computing facilities have been increasing exponentially, following Moore's law. Lately, due to physical limitations, we witnessed a downturn in this trend.  
It is now cheaper to buy cheap hardware, also called commodity hardware, in number than buying the largest possible machine. For the same computer power it is cheaper to use distributed architectures than very powerful single machines.
</p>
<h3> Distributed architectures are more secured</h3>
<p>
There are two important aspect in distributed architectures:
</p>
<ul>
<li><b>Partitioning</b>: data is cut into blocks of a given maximal size and spread across the cluster.</li>
<li><b>Replication</b>: data is replicated which means that we create copies of our partitions.</li>
</ul>

<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/partition_replication_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 5);return false;"/>
</center>
</div>

<p>
At first you may think that this increases the demand for storage, which is true, but remember that storage is very cheap while data is key to business today. This partition/replication increases the security of the data. We cannot assume that the crash of a machine is an extraordinary event: this is an event that is going to happen so we need to have copies of the data that are available at any time:
</p>
<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/redundancy_security_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 8);return false;"/>
</center>
</div>
<p>
<i>In this example, a document is spread on a cluster of 6 machines. It is very simplified but we have to have 3 machines down before not being able to access this document. The likeliness of this happening in a time so short that no one can react is low. Moreover tools are designed to see when a machine is down and take action so that the data is replicated on another machine so that we have all our replicas in the cluster.<br/>
In some companies where data is a very sensible issue, data is replicated inside data centers which are themselves replicated in different countries in order to avoid natural disasters (floods, earthquake, ...) or man made phenomena (attacks, war, ...).</i>
</p>
<h3>Distributed architectures speeds up computing</h3>
<p>
Partitioning data allows different machine to work on the same document at the same time. This is a fundamental  aspect of distributed architectures: since documents are spread over different machine, those machines can work in parallel at the same time. Moreover, since the data is already stored in small computable chunks on different machines, we do not need to move data from a machine to another: we just have to pick available machines that have the data. 
</p>
<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/distributed_computing_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 5);return false;"/>
</center>
</div>


<h3>But distributed architectures face some problems</h3>
<p>
As you might have guessed by now, the organisation of the cluster is very important and needs to be managed well: the cluster needs to know where pieces of data are stored, which machines are available to compute and how the results of a computation should be handled: this task is not trivial and that is why we have some tools such as <i>Hadoop</i> to perform this part. 
</p>
<p>
Moreover, the <b>CAP theorem</b> limits the possibilities of distributed systems: this theorem states that you cannot achieve perfect <b>availability</b> or perfect <b>consistency</b> of your data if your data is spread across a cluster.
</p>
<p>
<i>Think for example of a company that allows you to book airplane tickets. The availability of a given seat is going to be stored on different nodes at the same time. If Alice books the seat on a node, the information needs some time to be updated on the other nodes. If Bob wants to book the same seat, there are two possibilities:
</i>
</p>
<ul>
<li>you let Bob book the seat: the data is available but it is not coherent and you have to find a way to resolve this conflict.</li>
<li>you stop Bob from booking the seat: the data is no longer available but it will remain consisten.</li>
</ul>


<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/cap_theorem_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 5);return false;"/>
</center>
</div>

<p>
<b>CAP</b> actually stands for <b>Consistency</b>, <b>Availability</b> and <b>Partition Tolerance</b>: the theorem states that you cannot have the three at the same time. 
</p>
<h2>Map Reduce</h2>
<p>
MapReduce is a programming model used widely in distributed systems to perform calculations. It has been patented by Google in 2004. MapReduce relies on the emission of key-value pairs during a Map phase and the aggregation of the values by keys during the Reduce phase.
</p>
<p>
Every partition of the data undergoes the same transformation, emitting a set of key-value pairs. This is the <b>Map</b> phase. Those pairs are emitted to a machine based on the key of the pair.
</p>
<p>
For example every pair with the key <code>key_1</code> is sent to a specific machine no matter how many keys there are. There can be different keys on a single machine but every value corresponding to a key need to be on the same machine. Creating the key-value pairs is done during the <b>Map</b> phase while the sorting and dispatching of values is called the <b>Shuffle</b> phase.
</p>
<p>
Once all sent to the right machine, values are aggregated by key to get the final result: this is the <b>Reduce</b> phase. 
</p>
<h3>WordCount</h3>
<p>
To explain how Map Reduce works, the usual example is to take the `WordCount` task, e.g. computing the frequencies of words. It is the <code>print('hello world')</code> of MapReduce.
</p>
<p>
During the <b>Map</b> phase, the partitions of the text are lowered, tokenized and for each token, we emit the token as the key and 1 as the value.
</p>
<p>
The pseudo-code for this step is the following:
</p>

<code class="plaintext">
INPUT: data (list of strings)<br>
FOR EACH partition IN data:<br>
  token_list = TOKENIZATION(partition)<br>
  FOR EACH token IN token_list:<br>
    EMIT((token, 1))
</code>
<p>
Remember that the <b>Shuffle</b> phase forces all values corresponding to the same key to be on the same reducing machine. So on every machine used during the <b>Reduce</b> phase, we have a set of keys, corresponding to some tokens present in the text and for each key a list of ones of length corresponding to the number of instances of this token over the partitions. 
</p>
<p>
The pseudo-code for the <b>Reduce</b> is given in the following block: the code is given only for one key because during the <b>Reduce</b>, keys are treated totally independently.
</p>

<code class="plaintext">
INPUT: key (string), list_of_values (list of integers)<br>
s = 0<br>
FOR EACH i IN list_of_values:<br>
  s = s + i<br>
EMIT((key, s))
</code>

<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/mapreduce_wordcount_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 9);return false;"/>
</center>
</div>

<p>
The succession of a <b>Map</b>, <b>Shuffle</b> and <b>Reduce</b> phases is called a <b>Job</b>. We can combine multiple Jobs to perform complicated tasks. The number of reducers and mappers is up to the operator: if they is not a mapper by partition, some mappers will will take care of multiple partitions sequentially. On the other hand, having too many machines involved in an operation, some may not be available for other operations in the same time.
</p>
<h3>Advanced MapReduce: Combiners</h3>
<p>
You may have noticed that during the Map phase of Wordcount, we have emitted the same keys twice on the same mapper machine. This can be problematic as this may increase significantly the number of values that are emitted. 
</p>
<p>
We can aggregate those values during the <b>Map</b> phase using a <b>Combiner</b>: this can be considered as a <b>Reduce</b> phase in the mapper. If we take the example of Wordcount, the new pseudo-code for the <b>Map</b> is the following: 
</p>

<code class="plaintext">
INPUT: data (list of strings)<br>
temp = DICTIONARY(string, integer)<br>
FOR EACH partition IN data:<br>
  token_list = TOKENIZATION(partition)<br>
  FOR EACH token IN token_list:<br>
    IF token in temp.keys:<br>
       temp.set(token, 1 + temp.get(token))<br>
    ELSE:<br>
       temp.set(token, 1)<br>
FOR EACH token IN temp.keys:<br>
  EMIT((token, temp.get(token))
</code>

<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/wordcount_combiner_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 9);return false;"/>
</center>
</div>

<h3>Advanced MapReduce: Partitioners</h3>
<p>
During the <b>Shuffle</b> phase, the values are sent to specific machines according to the associated keys. The default rule is generally: 
</p>
<center><code> number of the reducer machine(key) = HASH_FUNCTION(key) MOD number of reducer machines</code></center>
<p>
This ensures an even distribution of keys over reducer nodes: each machine should receive about the same number of different keys. The issue with that is that in some particular cases, some keys can be over-represented. This can create a bottleneck which will slow down the execution of the jobs.  
</p>
<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/mapreduce_partitioners_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 3);return false;"/>
</center>
</div>

<p>
<i>In this example, the key <code>key1</code> is over-represented. <br/>If we let the default partitioner take care of the <b>Shuffle</b>, then one of the reducer is going to receive much more data to process than the other but if we take into account the a priori knowledge of the distribution of values, we can define a partitioner that will balance the workload over the reducers.</i>
</p>
<p>
Partitioners cannot be coded dynamically: you have to know a priori the balance of values by key to implement one that will balance evenly the workload.
</p>
<h2>Apache Hadoop</h2>
<p>
Apache Hadoop is an Open Source framework for distributed architectures. It was first released in 2006 by Yahoo! but is now an Apache Foundation project. It is a widely used tool in Big Data to manage distributed storage and computing. 
</p>
<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Hadoop.png" style="width:20%"/></center>
<h3>Hadoop components</h3>
<p>
Hadoop is mainly composed of 3 components:
</p>
<ul>
<li><b>HDFS</b> (Hadoop Distributed File System): HDFS is in charge a of partition, replication, orchestration of data. </li>
<li><b>YARN</b> (Yet Another Resource Negociator): YARN is in charge of orchestrating jobs and attributing resources.</li>
<li><b>Hadoop MapReduce</b>: Hadoop MapReduce is a Java API that implements the meta-class to perform MapReduce Jobs</li>
</ul>
<p>
While it is coded in Java, Hadoop allows also to use other programming languages by writing in and parsing the standard output directly. This allows us to code MapReduce Jobs in Python, Ruby, or any other language given that you can read from the standard output. This utility is called <b>Hadoop Streaming</b>.
</p>
<h4>HDFS</h4>
<p>
As stated before, <b>HDFS</b> is in charge of distributed storage. There are multiple daemons running in HDFS:
</p>
<ul>
<li><b>Datanode</b>: datanode is the daemon running on the slave nodes and that is in charge of allocating the interacting with the other daemons to make the data stored on the machine available.</li>
<li><b>Namenode</b>: the namenode is a daemon running on a master node: it contains meta-data about the location of the partitions. The namenode interacts with the datanodes to read or write data. </li>
<li><b>SecondaryNamenode</b>: secondary namenodes are daemons running on other master nodes. They are taking snapshots of the namenode at regular time intervals. If the namenode crashes, they are able to take over its role</li>
</ul>
<h4>YARN</h4>
<p>
<b>YARN</b> is in charge of orchestrating work and jobs. There are several daemons running within YARN:
</p>
<ul>
<li>The <b>Resource Manager</b> is the daemon in attributing resources to the different worker nodes. It runs on a master node.</li>
<li>The <b>Application Manager</b> is triggered by a client application. It is in charge of orchestrating jobs during the runtime of the application and asks the <b>Resource Manager</b> for resources.</li>
<li>The <b>Node Manager</b> is a daemon running on the worker nodes. It communicates with the <b>Resource Manager</b> at all time and with the <b>Application Manager</b> during the runtime of the application.</li>
<li><b>Containers</b> are the daemons triggered by the <b>Resource Manager</b> at the start of an application. They are in charge of the actual work. In fact, containers are <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">JVM</a> with allocated disk and memory.</li>
</ul>

<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/slider_yarn_working/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 6);return false;"/>
</center>
</div>
<p>
The code of an application contains information about the location of the files (in HDFS or locally), about the number of mappers and reducers and of course the code of Mappers, Combiners, Partitioners and Reducers. 
</p>
<p>
When it is launched, the <b>Resource Manager</b> triggers an <b>Application Manager</b> on one of the worker nodes. The <b>Application Manager</b> knows the resource needed (number of mappers and reducers) and asks the <b>Resource Manager</b> for those resources. 
</p>
<p>
The <b>Resource Manager</b> launches containers that includes a <b>JVM</b> and the code of the mappers and the reducers and communicates the address and ids of the containers to the <b>Application Manager</b>. 
</p>
<p>
Then, those containers communicate to the <b>Application Manager</b> which is in charge of orchestrating the jobs. The communication goes through the <b>Resource Manager</b> because <b>Hadoop</b> is based on a share-nothing model, e.g. worker nodes are only linked to the master nodes not one to each other. This leaves the <b>Resource Manager</b> available for other applications to be run on other nodes. 
</p>


<h2>Installing Hadoop</h2>
<p>
<i>In this part, we are going to install <b>Hadoop</b> in a pseudo-distributed mode. The components have already been downloaded but the setting must be done.</i>
</p>
<h3>Installation modes</h3>
<p>
3 installation modes are available:
</p>
<ul>
<li><b>Distributed mode</b>: This is the standard mode: the installation is spread over a cluster with worker nodes and slave nodes. </li>
<li><b>Pseudo-distributed mode</b>: This mode is a demonstration mode: we simulate the functioning in a distributed mode on a single machine by routing the pseudo-nodes to different ports of the machine. </li>
<li><b>Stand-alone mode</b>: This is also a demonstration mode where one machine is taken as a single node network and the local file system is used in place of the distributed one.</li>
</ul>
<h3>Configuration files</h3>
<p>
The installation files are located under the archive <code>/home/ubuntu/hadoop-2.7.3.tar.gz</code>. 
</p>
<p>
<i>It has been downloaded from <a href="https://hadoop.apache.org/releases.html">Hadoop</a> website. </i>
</p>
<p>
First, we need to decompress the archive:
</p>
<div class="sh code" id="code-1">
tar xvf /home/ubuntu/hadoop-2.7.3.tar.gz
mv ~/hadoop-2.7.3 ~/hadoop
</div>
<p>
To check that it indeed has been decompressed, you can use the command:
</p>
<div class="sh code" id="code-2">
ls -l | grep hadoop
</div>
<p>
You should see both the archive and the folder with the same name (except the extension).
</p>
<p>
Now we need to edit the <code>/home/ubuntu/.bashrc</code> file to give it paths to Hadoop files.
</p>
<p>
<i>We are going to use <code>nano</code> but you can use any text editor you want.</i>
</p>
<div class="sh code" id="code-3">
nano /home/ubuntu/.bashrc
</div>
<p>
Append those lines to the file:
</p>

<div class="sh code" id="code-4">
# HADOOP PATHS
export JAVA_HOME=/usr
export PATH=$PATH:/usr/bin/java
export HADOOP_HOME=/home/ubuntu/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export PATH=$PATH:$HADOOP_HOME/bin
</div>
<p>
Close the file and commit those changes with the <code>source</code>:
</p>

<div class="sh code" id="code-5">
source /home/ubuntu/.bashrc
</div>
<p>
We are going to check that <b>Java</b> and <b>Hadoop</b> are installed:
</p>

<div class="sh code" id="code-6">
java -version | grep openjdk
hadoop version | grep Hadoop
</div>
<p>
You should see the version of the two softwares.
</p>
<p>
We are now going to shape the cluster.
</p>
<p>
There are several files that need to be edited.
</p>
<h4><b>core-site.xml</b></h4>
<p>
This file contains settings for the <b>namenode</b>. The namenode address will be on local port 9000.
</p>
<p>
Open the file:
</p>

<div class="sh code" id="code-7">
nano /home/ubuntu/hadoop/etc/hadoop/core-site.xml
</div>
<p>
Within the tags <code> <configuration> </code>, paste the following lines:
</p>

<div class="xml code" id="code-67">
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>
</div>
<h4><b>hdfs-site.xml</b></h4>
<p>
This file contains information about how HDFS functions: we are going to choose a replication factor of 2, e.g. there will be 3 copies of each file. We will specify also a local directory to store namenode data and the datanode data. Finally, we need to tell him not to check for security clearance at each action.
</p>
<p>
Open the file:
</p>

<div class="sh code" id="code-8">
nano /home/ubuntu/hadoop/etc/hadoop/hdfs-site.xml
</div>
<p>
Within the tags <code> <configuration> </code>, paste the following lines:
</p>
<div class="xml code" id="code-68">
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
<property>
<name>dfs.permission</name>
<value>false</value>
</property>
<property>
<name>dfs.name.dir</name>
<value>/home/ubuntu/data/namenode_data</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>/home/ubuntu/data/datanode_data</value>
</property>
</div>
<h4><b>mapred-site.xml</b></h4>
<p>
In this file, we will simply state that the resource manager that will be used is YARN.
</p>
<p>
First, we need to copy/paste the template of this configuration file:
</p>

<div class="sh code" id="code-9">
cp /home/ubuntu/hadoop/etc/hadoop/mapred-site.xml.template /home/ubuntu/hadoop/etc/hadoop/mapred-site.xml
</div>
<p>
Open the file:
</p>

<div class="sh code" id="code-10">
nano /home/ubuntu/hadoop/etc/hadoop/mapred-site.xml
</div>
<p>
Within the tags <code> <configuration> </code>, paste the following lines:
</p>

<div class="xml code" id="code-69">
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</div>
<h4><b>yarn-site.xml</b></h4>
<p>
This files contains <b>YARN</b> settings. We simply tell him what <b>Java</b> classes should be used for the shuffle step:
</p>
<p>
Open the file:
</p>

<div class="sh code" id="code-11">
nano /home/ubuntu/hadoop/etc/hadoop/yarn-site.xml
</div>
<p>
Within the tags <code> <configuration> </code>, paste the following lines:
</p>

<div class="xml code" id="code-70">
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
</div>
<h4><b>hadoop-env.sh</b></h4>
<p>
In this file, we will just specify where <b>Hadoop</b> can find <b>Java</b>.
</p>
<p>
Open the file:
</p>

<div class="sh code" id="code-12">
nano /home/ubuntu/hadoop/etc/hadoop/hadoop-env.sh
</div>
<p>
Append this line to the file:
</p>

<div class="sh code" id="code-13">
export JAVA_HOME=/usr
</div>

<p>
One last thing to do is to generate SSH keys:
</p>

<div class="sh code" id="code-14">
ssh-keygen
</div>
<p>
Follow the default instructions (no password and default name) then add the key to the authorized keys:
</p>
<div class="sh code" id="code-15">
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
</div>
<h3>Initialization</h3>
<p>
First we need to format the namenode:
</p>

<div class="sh code" id="code-16">
hdfs namenode -format
</div>
<p>
You can check that a folder has been created by using the following command:
</p>

<div class="sh code" id="code-17">
ls -l /home/ubuntu/data
</div>
<p>
Now we can start the daemons for <b>HDFS</b>:
</p>

<div class="sh code" id="code-18">
/home/ubuntu/hadoop/sbin/start-dfs.sh
</div>
<p>
You can see which <b>Java</b> processes are running at anytime by using the command <code>jps</code>.
</p>
<p>
If you run this command here, you should see:
</p>
<ul>
<li>DataNode</li>
<li>NameNode</li>
<li>SecondaryNameNode</li>
</ul>
<p>
Now we can start <b>YARN</b> daemons:
</p>

<div class="sh code" id="code-19">
/home/ubuntu/hadoop/sbin/start-yarn.sh
</div>
<p>
If you run <code>jps</code>, you should see:
</p>
<ul>
<li>ResourceManager</li>
<li>NodeManager</li>
</ul>
<h3>HDFS practice</h3>
<p>
For this part, we have downloaded a book from <a href="https://www.gutenberg.org/">Project Gutenberg</a> library: The Adventures of Sherlock Holmes. It is a simple text file available on your local file system at <code>/home/ubuntu/datasets/books/sherlock_holmes.txt</code>. You can check the headers of the book with the following command:
</p>

<div class="sh code" id="code-20">
head -n 20 /home/ubuntu/datasets/books/sherlock_holmes.txt
</div>
<p>
Most of the commands that we are going to use are based on the same pattern: <code>hdfs dfs -...</code> and we simply to write those commands into the shell.
</p>
<p>
First, let's create a folder named <code>data</code> in our distributed file system:
</p>

<div class="sh code" id="code-21">
hdfs dfs -mkdir /data
</div>
<p>
You can check that the folder has indeed been created in the distributed file system and not in the local one by checking both systems at the root:
</p>
<b>Local file system</b>

<div class="sh code" id="code-22">
ls / | grep data
</div>
<b>HDFS</b>

<div class="sh code" id="code-23">
hdfs dfs -ls / | grep data
</div>
<p>
<code class="bash">hdfs dfs -ls</code> is used in the same way <code>ls</code> is used in a usual shell. HDFS is organised as any UNIX file sytem, starting from the root <code>/</code>.
</p>
<p>
We can also use the <code>-R</code> argument to have a recursive view of the folders: <code class="bash">hdfs dfs -ls -R /</code>.
</p>
<p>
Notice that we always need to specify the absolute path as there are no current directory in <b>HDFS</b>.
</p>
<p>
We are now about to put our book into <b>HDFS</b>:
</p>

<div class="sh code" id="code-24">
hdfs dfs -put /home/ubuntu/datasets/books/sherlock_holmes.txt /data/
</div>
<p>
We can also use <code>-copyFromLocal</code>
</p>
<div class="sh code" id="code-25">
hdfs dfs -copyFromLocal /home/ubuntu/datasets/books/sherlock_holmes.txt /data/
</div>
<p>
The main difference between those two commands is that <code>-put</code> can handle multiple files at once while <code>-copyFromLocal</code> cannot.
</p>
<p>
The syntax is:
</p>
<div class="sh code" id="code-26">
hdfs dfs -put local_file_path distributed_path
</div>
<p>
This command is very similar to <code>cp</code> or <code>scp</code>.
</p>
<p>
We can check that the file is indeed here:
</p>

<div class="sh code" id="code-27">
hdfs dfs -ls -R /
</div>
<p>
Or print its content using <code>-cat</code>:
</p>
<div class="sh code" id="code-28">
hdfs dfs -cat /data/sherlock_holmes.txt
</div>
<p>
The contrary can be done using <code>-get</code>:
</p>
<div class="sh code" id="code-29">
hdfs dfs -get distributed_path local_path
</div>
<p>
Of course we can do a lot of the usual commands of a filesystem:
</p>

<div class="sh code" id="code-30">
# removing a file
hdfs dfs -rm /path/to/file
# removing a folder
hdfs dfs -rm -r /path/to/folder
# copying a file from and to the distributed file system
hdfs dfs -cp /path/to/file1 /path/to/file2
# copying a folder from and to the distributed file system
hdfs dfs -cp -r /path/to/folder1 /path/to/folder2
# moving  a file from and to the distributed file system
hdfs dfs -mv /path/to/file /new/path/to/file
# moving  a folder from and to the distributed file system
hdfs dfs -mv -r /path/to/folder /new/path/to/folder
</div>
<p>
We can also use regular expressions to address multiple files at the same time.
</p>
<h4>User Interface</h4>
<p>
HDFS has also a way to visualize the file system information: if you open a window on the 50070 port, you should be able to see the UI.<br>
For example, you can see your files in <code>Utilities>Browse the file system </code>.
</p>


<h4>Exercise</h4>
<p>
<i>In the folder <code>/home/ubuntu/books</code> there are <code>moby_dick.txt</code> and <code>alice.txt</code>. Try to do the following:
</i></p>
<ul>
<li>create a folder <code>/data/books</code>.</li>
<li>copy both books from local file system to this folder.</li>
<li>copy the file <code>sherlock_holmes.txt</code> on <b>HDFS</b> to this new folder. </li>
<li>print the content of those files.</li>
<li>make a copy of the folder <code>/data/books_backup</code>.</li>
<li>make a copy of <code>moby_dick.txt</code> named <code>moby_dick2.txt</code> in the same folder.</li>
<li>delete those two files from this folder in one command using a regular expression.</li>
<li>delete the folder <code>/data/books</code>.</li>
</ul>
<h3>MapReduce practice</h3>
<p>
We are going to execute a <b>MapReduce</b> job performing wordcount.
</p>
<h4>Hadoop MapReduce</h4>
<p>
In this first part, we will perform a <b>WordCount</b> on <code>sherlock_holmes.txt</code> using the <b>Hadoop MapReduce Java</b> library. The application is already coded and compiled in the file <code>wordcount.jar</code> but it is available <a href="lien-vers-wordcount.java">here</a>. Feel free to read this file to get the pattern on <b>MapReduce</b> job implementation. You'll note that we did not specify a number of mappers and reducers, so there will be only one of each by default.
</p>
<p>
To run this file, the syntax is the following:
</p>

<div class="sh code" id="code-31">
hadoop jar /home/ubuntu/code/wordcount.jar WordCount /data/sherlock_holmes.txt /output_wordcount
</div>
<p>
More generally, to perform a <b>MapReduce</b>, we can do the following:
</p>

<div class="sh code" id="code-32">
hadoop jar /path/to/jar/file ClassName ...
</div>
<p>
Here the two arguments are the input file and an output folder. Note that this folder should not exist before running this code. <br>
Once run, you can see that Hadoop is very wordy: there are a lot of information and it is sometimes difficult to read through it.
</p>
<p>
Now we can check the result of our job:
</p>

<div class="sh code" id="code-33">
hdfs dfs -ls -R /output_wordcount
</div>
<p>
As you can see a file named <code>part-r-00000</code> has been created. It contains the result of the Job.
</p>

<div class="sh code" id="code-34">
hdfs dfs -cat /output_wordcount/part-r-00000
</div>
<p>
This name corresponds to the fact that we have the output of a single reducer. If we had multiple reducers, we would have different files corresponding to the different partitions of our results with names incremented by 1 for each reducer.
</p>
<h3>Hadoop Streaming with Python</h3>
<p>
As stated before, <b>Hadoop Streaming</b> allows us to perform <b>MapReduce</b> jobs using other languages than <b>Java</b>.
</p>
<p>
In this part we are going to run a <b>MapReduce</b> job using <b>Python</b>.<br>
In the folder <code>/home/ubuntu/code</code>, you can find <code>mapper.py</code> and <code>reducer.py</code>. Feel free to read their content: we are basically doing the same thing as before but with <b>Python</b> we are only printing out results in the standard output. Note also that we need to specify which command is used at the beginning of the file.
</p>
<p>
Before running the command, we are just going to alias the command: this will ease the command.
</p>
<p>
Open <code>.bashrc</code>:
</p>

<div class="sh code" id="code-35">
nano /home/ubuntu/.bashrc
</div>
<p>
Append the following lines to the file:
</p>

<div class="sh code" id="code-36">
alias hadoop_streaming="hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar"
</div>
<p>
Do not forget to commit the changes to this file with <code>source ~/.bashrc</code>.
</p>
<p>
You can run the job by doing the following command:
</p>

<div class="sh code" id="code-37">
hadoop_streaming \
-file ~/code/mapper.py \
-mapper ~/code/mapper.py \
-file ~/code/reducer.py \
-reducer ~/code/reducer.py \
-input /data/sherlock_holmes.txt \
-output /output_wordcount_streaming
</div>
<p>
We have to specify the code files twice because they are not hosted on the distributed file system. Moreover the output argument is still a non-existing folder in <b>HDFS</b>.
</p>
<p>
You can check the results:
</p>

<div class="sh code" id="code-38">
hdfs dfs -ls -R /output_wordcount_streaming
hdfs dfs -cat /output_wordcount_streaming/part-00000
</div>
<p>
The code is a bit simpler than before so the results are a bit rougher. Yet we get the same idea.
</p>
<h4>Exercise</h4>
<p>In this part, we will use the file <code>sentiment.csv</code> in the folder <code>/home/ubuntu/datasets</code>. This file contains two columns: the first represents the text of tweets while the second represents the sentiment (0 is for negative, 1 for positive).
</p>
<p>
You shall do the following:
</p>
<ul>
<li>put the file <code>sentiment.csv</code> into <b>HDFS</b>.</li>
<li>create a mapper and a reducer scripts in <b>Python</b> that will count the number of positive and negative tweets.</li>
<li>run this job and get its output into a folder <code>/output_count</code>.</li>
<li>import this file from <b>HDFS</b> into the <code>/home/ubuntu/datasets</code> folder.</li>
</ul>

<h3>Conclusion on Hadoop</h3>
<p>
In this part, we have seen the importance of distributed systems and the advantages it presents compared to classical architectures. We have also seen how to code in a framework like <b>MapReduce</b> and how to use specifically <b>Hadoop</b>.
</p>
<p>
The whole <b>Hadoop</b> framework is interesting as it is one of the main cornerstone of many tools: we shall see some of them in the following lessons: Hive, Pig, Spark, HBase, ...
</p>
<h2>Hive</h2>
<h3>Introduction</h3>
<p>
Hive is a very interesting element of the Data Engineer toolbox. It provides a SQL-like interface for tabular data. Development started in 2007 at Facebook and it was first released as an Open Source project in 2015. Today, Hive is managed by the Apache Foundation.
</p>
<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Hive.png" style="width:20%"/></center>
<p>
Hive is an abstraction of a relational database management system (RDBMS) but relies on Hadoop for distribution (repartition, partition, replication, …) and computation. It uses HQL (for Hive Query Language) which is a SQL like query language.
</p>
<p>
Data is stored as flat files in HDFS and queries are actually translated from HQL to MapReduce jobs using YARN.
</p>
<h3> A word on execution planning</h3>
<p>
There is an important component of Hive, the metastore. The metastore is a local relational database used to store metadata on the location of the flat files in HDFS and the schema of the tables.
</p>
<p>
The other important components of Hive are:
</p>
<ul>
<li><b>CLI</b> (Command Line Interface): interface for the user to type in queries.</li>
<li><b>driver</b>: daemon that interprets queries and is the interface between the CLI, the compiler and the execution engine.</li>
<li><b>compiler</b>: daemon in charge of parsing the queries, dealing with the metadata needs and creating the DAG of the execution.</li>
<li><b>execution engine</b>: daemon in charge of interpreting the queries into MapReduce jobs and making the interface with Hadoop.</li>
</ul>
<p>
The mechanics are shown in the following figure:
</p>
<div class="slider">
<center>
<img src="https://raw.githubusercontent.com/pauldechorgnat/de_help/master/static/hive_execution_slider/image0.png" style="cursor:pointer; margin-left: auto; margin-right: auto; width:80%; height:auto;"
onclick="next_slide(this, 14);return false;"/>
</center>
</div>

<div class="alert-info">
<span class="glyphicon glyphicon-info-sign"> <b>DAGs</b> (Directed Acyclic Graphs) are a very important concept of execution planning. A DAG is a graph whose nodes are individual tasks and edges represent dependencies between those tasks. The goal of a DAG is to be optimized: tasks that can be performed at once are regrouped and independent tasks are set to be run in parallel.
</span></div>
<h3> Partitioning </h3>
<p>
As stated before, Hive is not really a <b>RDBMS</b>: it acts like it but it is truly HDFS that is storing the files. In fact, <b>Hive</b> is just an interface that allows to make SQL-like queries on very large datasets: the ability to distribute files on several machines, hence the larger capacities, is what make Hive more interesting than a regular RDBMS.
</p>
<p>
But some queries may take a while to perform because of the size of the datasets... In order to speed up queries, Hive has been implemented with the ability to partition data on different columns. For example, if you imagine a dataset with clients. We could partition it on the nationality of our clients. This means that we will have one file for each nationality. This file can be partitioned but each partition will contain one and only one nationality. This means that queries based on nationality will be highly sped up as there are no need to check every record.
</p>
<p>
You can have multiple levels of partitioning (nationality, age group, gender, ...).
</p>
<h3>Installation</h3>
<p>
In this part, we will install Hive. We will have to configure Hive and install an external relational database to serve as Metastore. There are multiple available choices but we will use <a href="https://db.apache.org/derby/">Apache Derby</a>.
</p>
<p>
First we need to decompress the archive files (to ease the use of these folders we will rename them):
</p>
<div class="sh code" id="code-39">
tar xvf ~/apache-hive-2.3.6-bin.tar.gz
tar xvf ~/db-derby-10.13.1.1-bin.tar.gz
mv ~/apache-hive-2.3.6-bin ~/hive
mv ~/db-derby-10.13.1.1-bin ~/derby
</div>
<p>
Then we need to update the paths in the <code>~/.bashrc</code> file:
</p>
<div class="sh code" id="code-40">
nano ~/.bashrc
</div>
<p>
In this file we will paste the following lines:
</p>

<div class="sh code" id="code-41">
# PATHS FOR HIVE
export HIVE_HOME=/home/ubuntu/hive
export HIVE_CONF_DIR=$HIVE_HOME/conf
export PATH=$PATH:$HIVE_HOME/bin
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*
# PATHS FOR DERBY
export DERBY_HOME=/home/ubuntu/derby
export PATH=$PATH:$DERBY_HOME/bin
export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar
</div>
<p>
Save the changes, close the file and commit the changes to the system with <code>source ~/.bashrc</code>.
</p>
<p>
Then we need to create folders in HDFS where data will be stored and change their rights (within HDFS) to give writing priviledges to group user.
</p>
<div class="sh code" id="code-42">
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -mkdir /tmp
hdfs dfs -chmod g+w /user/hive/warehouse
hdfs dfs -chmod g+w /tmp
</div>
<p>
There are some configuration files to change too: copy the <code>$HIVE_HOME/conf/hive-env.sh.template</code> file and change its name.
</p>
<div class="sh code" id="code-43">
cp $HIVE_HOME/conf/hive-env.sh.template $HIVE_HOME/conf/hive-env.sh
nano $HIVE_HOME/conf/hive-env.sh
</div>
<p>
Add the line: <code>export HADOOP_HOME=/home/ubuntu/hadoop</code>, save and close the file. We need also to change the file containing the whole configuration of Hive:
</p>
<div class="sh code" id="code-44">
cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml
nano $HIVE_HOME/conf/hive-site.xml
</div>
<p>
This file contains a lot of information, a lot of parameters... To navigate through this file, you can use <code>ctrl + w</code> to look for a particular term.
</p>
<p>
Here we need to make the following changes:
</p>
<ul>
<li> <code>javax.jdo.option.ConnectionURL</code> property: <code>jdbc:derby:;databaseName=/home/ubuntu/hive/bin/metastore_db;create=true</code> value</li>
<li><code>hive.exec.local.scratchdir</code> property: <code>/tmp/hivedir</code> value </li>
<li><code>hive.downloaded.resources.dir</code> property: <code>/home/ubuntu/hive/tmp/${hive.session.id}_resources</code> value</li>
</ul>
<p>
We need to create a last file for Hive:
</p>

<div class="sh code" id="code-45">
nano $HIVE_HOME/conf/jpox.properties
</div>
<p>
In this file we need to paste the following lines:
</p>
<div class="sh code" id="code-46">
javax.jdo.PersistenceManagerFactoryClass=org.jpox.PersistenceManagerFactoryImpl
org.jpox.autoCreateSchema=false
org.jpox.validateTables=false
org.jpox.validateColumns=false
org.jpox.validateConstraints=false
org.jpox.storeManagerType=rdbms
org.jpox.autoCreateSchema=true
org.jpox.autoStartMechanismMode=checked
org.jpox.transactionIsolation=read_committed
javax.jdo.option.DetachAllOnCommit=true
javax.jdo.option.NontransactionalRead=true
javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.ClientDriver
javax.jdo.option.ConnectionURL=jdbc:derby://hadoop1:1527/metastore_db;create=true
javax.jdo.option.ConnectionUserName=APP
javax.jdo.option.ConnectionPassword=mine
</div>
<p>
We also need a directory for derby:
</p>
<div class="sh code" id="code-47">
mkdir $DERBY_HOME/data
</div>
<p>
Finally, the last step of the installation the initialization of the derby database:
</p>
<div class="sh code" id="code-48">
$HIVE_HOME/bin/schematool -dbType derby -initSchema
</div>
<p>
Now Hive should be installed and you should be able to launch it through the <code>hive</code> command (to leave the CLI, type in <code>exit</code>).
</p>
<h3>Practice</h3>
<p>
In this part, we will try to show you how Hive works. First we need to put a file into HDFS:
</p>
<div class="sh code" id="code-49">
hdfs dfs -put /home/ubuntu/datasets/movies/ratings.csv /ratings.csv
hdfs dfs -put /home/ubuntu/datasets/movies/movies.csv /movies.csv
</div>
<p>
Check that the file is indeed uploaded:
</p>
<div class="sh code" id="code-50">
hdfs dfs -ls /
</div>
<p>
Next, open Hive CLI with <code>hive</code>. As with a lot of relational databases, you can use <code>SHOW DATABASES;</code> to see what databases are already in Hive. We will need to use one of them to carry on. Here we will of course need to create one:
</p>
<div class="sql code" id="code-1000">
CREATE DATABASE my_hive_db;
USE my_hive_db;
</div>
<p>
We should see what the tables in <code>my_hive_db</code> are with <code>SHOW TABLES;</code>: there are no tables yet.
</p>
<p>
To create a table the syntax is very similar to regular RDBMS. Here we want to create a table <code>ratings</code>.
</p>

<div class="sql code" id="code-77">
CREATE TABLE ratings
(userid int,
movieid int,
rating double,
time_stamp int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ","
LINES TERMINATED BY "\n"
STORED AS TEXTFILE;
</div>
<p>
Note that we need to specify some things that are unusual at this step: how data is and will be stored.
</p>
<p>
To load data into the table, the syntax is similar to regular RDBMS syntax:
</p>

<div class="sql code" id="code-78">
LOAD DATA INPATH '/ratings.csv'
INTO TABLE ratings;
</div>
<p>
Notice that the path to the file is the HDFS path. If we want to load a file from the local file system, we can use the following tweak: <code>LOAD DATA LOCAL INPATH ... </code>.
</p>
<p>
Once you have loaded the file, quit Hive with the <code>exit</code> command.
</p>
<p>
To illustrate how Hive works we are going to list the files at the root of HDFS:
</p>
<div class="sh code" id="code-51">
hdfs dfs -ls /
</div>
<p>
The file <code>ratings.csv</code> has disappeared. In fact, it has been moved into the <code>/user/hive/warehouse</code>. We can list recursively the content of this folder:
</p>
<div class="sh code" id="code-52">
hdfs dfs -ls -R /user/hive/warehouse/
</div>
<p>
You should see that a folder <code>db.my_hive_db</code> has been created. It contains a folder <code>ratings</code> that correspond to the table in question and its content is a file <code>part-00000</code>. If the file was larger we could have had multiple partitions.
</p>
<p>
We can print the content of this file:
</p>

<div class="sh code" id="code-53">
hdfs dfs -cat /user/hive/warehouse/db.my_hive_db/ratings/part-00000
</div>
<p>
The content of the file has not been changed.
</p>
<p>
Now let's return to Hive CLI: <code>hive</code>.
</p>
<p>
We are going to create a table to illustrate the Hive partitioning abilities. To create a partitioned table, we need to specify on which variable we want to partition:
</p>

<div class="sql code" id="code-79">
USE my_hive_db;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
CREATE TABLE ratings_part
(userId int, movieId int, times int)
PARTITIONED BY (rating double)
STORED AS TEXTFILE;
</div>
<p>
We want to partition the table on the <code>rating</code> column so we have to specify it in last position. To insert data into the table, we will use a query on the <code>ratings</code> table:
</p>
<div class="sql code" id="code-80">
INSERT INTO TABLE ratings_part
PARTITION (rating)
SELECT userId, movieId, time_stamp, rating
FROM ratings
</div>
<p>
The <code>rating</code> column is in last place because of the definition of the table <code>ratings_part</code>. Once these commands are launched into Hive CLI, return to the shell and launch the following command to inspect the content of HDFS:
</p>
<div class="sh code" id="code-54">
hdfs dfs -ls -R /user/hive/warehouse/my_hive_db.db
</div>
<p>
You can see that the folder <code>ratings_part</code> is subdivided in multiple subfolders corresponding to the different partitions of the column <code>rating</code>.
</p>
<p>
We will just try to see if there are differences between the two tables: connect to the Hive CLI and run the following command:
</p>

<div class="sql code" id="code-81">
USE my_hive_db;
SELECT COUNT(*) AS nb_ratings, rating FROM ratings
GROUP BY rating;
</div>
<p>
Once you have seen the time taken to run the query, run the following:
</p>

<div class="sql code" id="code-82">
SELECT COUNT(*) AS nb_ratings, rating FROM ratings_part
GROUP BY rating;
</div>
<p>
You should see a difference between the two tables: in the first case, we have to check every line while in the second case, the ratings are already stored in different partitions, so the query takes less time.
</p>
<p>
We are not going to go deeper on Hive for the moment: this is an interesting tool to use the power and the accessibility of RDBMS in a distributed framework.
</p>
<h2>Pig</h2>
<p>
Apache Pig is a useful tool to perform data manipulation in a distributed environment. It has its own language: <b>Pig Latin</b> and abstracts MapReduce jobs. The idea behind Pig is to simplify the use of MapReduce: for example, the wordcount jobs take about 60 lines in <b>Java</b> while it takes  5 lines with Pig.
</p>
<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Pig.png" style="width:20%"/></center>
<p>
Moreover, Pig is using lazy evaluations and DAGs to optimize the queries: the queries are passed through a parser, an optimizer, a compiler and then an execution engine to run the queries. Pig can use different orchestrators (Yarn, Spark, Tez, ...)
</p>
<h3>Installation</h3>
<p>
In this part, we are going to install Pig.
</p>
<p>
First extract the archive:
</p>

<div class="sh code" id="code-55">
tar xvf ~/pig-0.17.0.tar.gz
mv ~/pig-0.17.0 ~/pig
</div>
<p>
We simply need to add paths to the Pig in the <code>~/.bashrc</code> file.
</p>

<div class="sh code" id="code-56">
# PIG PATHS
export PIG_HOME=/home/ubuntu/pig
export PATH=$PATH:$PIG_HOME/bin
export PIG_CLASSPATH=$HADOOP_HOME/etc/hadoop
# HCATALOG PATHS
export HCAT_HOME=$HIVE_HOME/hcatalog
export PATH=$PATH:$HCAT_HOME/bin
</div>
<p>
Next, we need to launch Hadoop Job History Server:
</p>
<div class="sh code" id="code-57">
$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver
</div>
<p>
Now to check that Pig is working, just run <code>pig</code>. This should open Pig CLI <code>grunt</code>. To leave this interface, you can use <code>quit</code>.
</p>
<h3>Pig Latin</h3>
<p>
The basic data structure is called a relation in Pig. It is basically a list of named objects. Those objects are of the same type: this means that the structure needs a schema. To load data, we use the <code>LOAD</code> keyword as following:
</p>
<div class="pig code" id="code-1001">
movies = LOAD 'hdfs:///movies.csv' USING PigStorage(',') AS (id:int, title:chararray, genres:chararray);
</div>
<p>
Except if you have missed the right path to the file, there should not be a lot of information printed when this command is launched: it is because of the lazy evaluation.
</p>
<p>
Here the schema is introduced by the <code>AS</code> keyword: here we have a column of integers and two columns of strings.
</p>
<p>
The keyword <code>USING</code> introduces the function used to load data. There are many different functions corresponding to different file types. We will see later how to load directly data from Hive with a different function.
</p>
<p>
If we want to execute the operations completely of partly, we can use the following keywords:
</p>
<ul>
<li><code>describe movies;</code>: this line returns the schema of the relation.</li>
<li><code>illustrate movies;</code>: this line returns a line of the relation as an example.</li>
<li><code>explain movies;</code>: this line returns the different steps in the DAGs to get to this relation.</li>
<li><code>dump movies;</code>: this line prints out the content of the relation.</li>
<li><code>STORE moveis INTO 'hdfs:///movies2.csv' USING PigStorage(',');</code>: this line writes the content of the relation into a file in HDFS.</li>
</ul>
<p>
To perform map operations, we can use the <code>FOREACH</code> and <code>GENERATE</code> keywords. For example, if we want to create a relation with only the column <code>genre</code>, we can use:
</p>

<div class="pig code" id="code-71">
genres_list = FOREACH movies GENERATE genres AS genres_of_the_movie;
</div>
<p>
This line of code creates a relation named <code>genres_list</code> with the following schema <code>(genres_of_the_movie: chararray)</code>.
</p>
<p>
We can also perform flatmap operations (eg generating multiple lines per line), using the <code>FLATTEN</code> keyword:
</p>

<div class="pig code" id="code-72">
unique_genre_list = FOREACH genres_list GENERATE FLATTEN(STRSPLIT(genres_of_the_movie, '\\|')) as genre;
</div>
<p>
As a reminder, the content of the column <code>genres_of_the_movie</code> is a pipe (<code>|</code>) separated file.
</p>
<p>
We can perform Group By operations with <code>GROUP</code>:
</p>

<div class="pig code" id="code-73">
genres_group = GROUP unique_genre_list BY genre;
</div>
<p>
If you illustrate the content of the relation, you will see that it is similar to a dictionary with the values being list of keys: for example, <code>{'comedy': ['comedy', 'comedy'], ...}</code>.
</p>
<p>
To reduce this relation, we can use <code>FOREACH</code> once again:
</p>

<div class="pig code" id="code-74">
genres_count = FOREACH genres_group GENERATE group AS genre, COUNT(unique_genre_list) AS nb_movies;
</div>
<p>
Notice that in this example, we have done something very similar to the wordcount example.
</p>
<p>
There are also other keywords that can be used (that are very similar to <code>SQL</code>):
</p>
<ul>
<li><code>FILTER ... BY ...</code></li>
<li><code>ORDER ... BY ...</code></li>
<li><code>JOIN ... BY ... LEFT ... BY </code></li>
<li><code>UNION ..., ...</code></li>
<li>...</li>
</ul>
<p>
One other interesting thing is the ability to use user defined functions (UDFs). UDFs are specified in a Java archive and must follow certain <a href="https://pig.apache.org/docs/latest/udf.html">rules</a>. To import a function, you can use:
</p>

<div class="pig code" id="code-75">
REGISTER 'path/to/the/jar/file';
DEFINE alias_name full.name.of.the.function;
</div>
<p>
Pig is interesting because there are a lot of different pre-defined functions (maths, text, ...) and you can define your own functions. Moreover it can be linked easily to Hive. For this you need to launch <code>grunt</code> with the commande <code>pig -useHCatalog</code> and use the following syntax while loading data:
</p>

<div class="pig code" id="code-76">
relation_name = LOAD 'database_name.table_name'
USING org.apache.hive.hcatalog.pig.HCatLoader();
</div>
<p>
And to store data directly in Hive, we can use the <code>org.apache.hive.hcatalog.pig.HCatStorer()</code>.
</p>
<p>
In some aspects, Hive and Pig are quite similar. They both can be used for Data Manipulation but the first one can also store data as an abstracted relational database. This is the main difference but the two tools also do not have the syntax. Basically, Pig is more advanced in terms of customization but it is really up to the user of these tools.
</p>
<h2>Sqoop</h2>
<p>
Finally, this last part will deal with Apache Sqoop: this tool can be used for ETL (Extract Transform Load) pipelines. Basically, Sqoop is used to transfer data from relational databases to HDFS (and the other way around).
</p>
<center><img src="https://github.com/pauldechorgnat/de_help/raw/master/static/Sqoop.png" style="width:20%"/></center>
<p>
This can be very interesting to use the power of distributed computing power on regular databases, or to store transformed data into a datalake based on Hadoop.
</p>
<h3>Installation</h3>
<p>
First we must decompress the archive:
</p>
<div class="sh code" id="code-58">
tar xvf ~/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
mv sqoop-1.4.7.bin__hadoop-2.6.0 ~/sqoop
</div>
<p>
Open the file <code>~/.bashrc</code> and paste the following lines:
</p>
<div class="sh code" id="code-59">
export SQOOP_HOME=/home/ubuntu/sqoop
export PATH=$PATH:$SQOOP_HOME/bin
</div>
<p>
Save, close and commit (using <code>source ~/.bashrc</code>) this file.
</p>
<p>
Next we need to change a few lines in the configuration file:
</p>
<div class="sh code" id="code-60">
cp $SQOOP_HOME/conf/sqoop-env-template.sh $SQOOP_HOME/conf/sqoop-env.sh
nano $SQOOP_HOME/conf/sqoop-env.sh
</div>
<p>
Paste those lines:
</p>
<div class="sh code" id="code-61">
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
</div>
<p>
Finally, there is a file that we need to put into the <code>$SQOOP_HOME/lib</code> folder:
</p>

<div class="sh code" id="code-62">
cp /usr/share/java/mysql* $SQOOP_HOME/lib
</div>
<h3>Practice Sqoop</h3>
<p>
The main features of Sqoop are pretty easy to use. On your machine, we have installed <b>MySQL</b>: it contains a database named <code>my_db</code> and a table called <code>movies</code>.
</p>
<p>
To import data from MySQL to HDFS, the syntax is the following:
</p>

<div class="sh code" id="code-63">
sqoop import --connect jdbc:mysql://localhost:3306/my_db \
--username hduser \
--P \
--table movies \
--m 1 \
--target-dir /data/movies \
--fields-terminated-by '^'
</div>
<p>
We are using <code>sqoop import</code> to import data and the arguments are:
</p>
<ul>
<li><code>--connect</code>: we have the Java DataBase Connector <code>jbdc</code>, the name of the RDBMS <code>mysql</code>, the open port of the RDBMS <code>localhost:3306</code> and the database name <code>my_db</code>.</li>
<li><code>--username</code>: indicates the username in MySQL</li>
<li><code>--P</code>: indicates that we want to type in our password. Here the password is <code>password</code>.</li>
<li><code>--table</code>: name of the table we want to import <code>movies</code>.</li>
<li><code>--m</code>: the number of mappers to use.</li>
<li><code>--target-dir</code>: the name of the directory to put the data in.</li>
<li><code>--fields-terminated-by</code>: the character to use to separate the fields</li>
</ul>
<p>
Once you have run this command, you can check if the data has been imported into HDFS.
</p>
<div class="sh code" id="code-64">
hdfs dfs -ls /data/movies
hdfs dfs -cat /data/movies/part-m-00000
</div>
<p>
Instead of importing the whole table, you can also import only some of it using the argument <code>--query</code>:
</p>
<div class="sh code" id="code-65">
sqoop import --connect jdbc:mysql://localhost:3306/my_db \
--username hduser \
--P \
--query "SELECT * FROM movies WHERE movies.genres = 'comedy' AND \$CONDITIONS" \
--m 1 \
--target-dir /data/comedies \
--fields-terminated-by '^'
</div>
<p>
This is very useful for ETL pipelines. Notice that we need to specify a condition with <code>WHERE</code> and the condition has to contain <code>\$CONDITIONS</code>.
</p>
<p>
Arguments for the export from HDFS to MySQL are approximately the same:
</p>
<ul>
<li>use <code>sqoop export</code> instead of <code>sqoop import</code>.</li>
<li>use <code>table</code> to indicate the table in which to place the data.</li>
<li>use <code>--fields-terminated-by</code> to indicate how to parse the file.</li>
<li>use <code>--export-dir</code> to indicate what is the path to the file to export in HDFS.</li>
</ul>
<p>
One other feature that is interesting is the ability to import data directly into Hive:
</p>
<div class="sh code" id="code-66">
sqoop import --connect jdbc:mysql://localhost:3306/my_db \
--username hduser --P \
--table movies \
--hive-import \
--hive-table my_db.movies \
-m 1
</div>
<p>
The table does not need to be created as Sqoop will create it during this command. If we want to import only a query, use the <code>--query</code> argument as before and if you want to import all the tables, change <code>sqoop import</code> to <code>sqoop import-all-tables</code> and <code>hive-table</code> to <code>hive-database</code>.
</p>
<h1>Conclusion</h1>
<p>
In this lecture, we have seen the advantages of distributed over classical architecture: security, computation and storage capacities enhanced, cost-efficient, ... Hadoop is the main framework in this domain as it is Open Source and that a lot of tools are based on its source code: Hive and Pig are useful to manipulate data, create datasets that can be used by Data Scientists and Data Analysts and provide interfaces that are more useful while Sqoop is used to do ETL pipelines.
</p>

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
			integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
			<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
			integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
			crossorigin="anonymous"></script>
			<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
			integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
			crossorigin="anonymous"></script>
			<script src="https://cdnjs.cloudflare.com/ajax/libs/ace/1.3.3/ace.js" type="text/javascript"
			charset="utf-8"></script>
<script>
	change_code = function(language='python'){
		var regular_expression = new RegExp(/^(\t)+/);
		var divs = document.getElementsByClassName(language);
		for (var i=0; i<divs.length; i++){

            var element = divs[i];
            var lines = element.innerHTML.split('\n');
            var nb_lines = lines.length;
            text = element.innerHTML;

            element.style.height = ((nb_lines) * 16) + 'px';
            var editor = ace.edit(divs[i].id);
            text = text.split('&amp;').join('&');
            text = text.split('&gt;').join('>');
            text = text.split('&lt;').join('<');
            editor.setValue(text.substring(1), -1);
            editor.setTheme("ace/theme/monokai");
            editor.session.setMode("ace/mode/" + language);
            // editor.session.highlight()
            editor.setFontSize('16px');
            // editor.session.indentRows(startRow=0, endRow=nb_lines, indentString='\t');

         	// beautify.beautify(editor.session);
         	editor.setReadOnly(true);
         }
     }
     var languages = ['python', 'sql', 'sh', 'json', 'dockerfile', 'yaml', 'xml', 'pig'];
     for (var l=0; l<languages.length; l++){
     	change_code(languages[l]);
     }




 </script>
 <script>
 	next_slide = function(element, number_of_images){
 		var image_name = element.src.match('image[0-9]+\\.png')[0];
 		var new_image_number = (Number.parseInt(image_name.match('[0-9]+')) + 1) % number_of_images;
 		var new_image_path = 'image' + new_image_number + '.png';
 		element.src = element.src.replace(RegExp('image[0-9]+\\.png'), new_image_path);
 		return false;
 	}



 </script>
 <script>
 	function copyText(element) {
 		var range, selection, worked;

 		if (document.body.createTextRange) {
 			range = document.body.createTextRange();
 			range.moveToElementText(element);
 			range.select();
 		} else if (window.getSelection) {
 			selection = window.getSelection();
 			range = document.createRange();
 			range.selectNodeContents(element);
 			selection.removeAllRanges();
 			selection.addRange(range);
 		}

 		try {
 			document.execCommand('copy');
 			alert('text copied');
 		}
 		catch (err) {
 			alert('unable to copy text');
 		}
 	}



 </script>

</div>
</body>